{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "##> import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "#> import flower\n",
    "import flwr as fl\n",
    "\n",
    "#> import custom libraries\n",
    "from src.load import load_df_to_dataset\n",
    "from src.EAE import EvidentialTransformerDenoiseAutoEncoder, evidential_regression\n",
    "from src.client import train_and_evaluate_local, evaluate_saved_model\n",
    "from src.datasets import TrajectoryDataset, clean_outliers_by_quantile\n",
    "from src.plot import plot_loss, plot_tsne_with_uncertainty, plot_uncertainty\n",
    "\n",
    "#> torch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#> Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "#plt.style.use(['science', 'grid', 'notebook'])  # , 'ieee'\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "#%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Define the dataset catalog\n",
    "assets_dir = root_dir.parents[3] / 'aistraj' / 'bin'/ 'tvt_assets'\n",
    "assets_dir = assets_dir.resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "    \n",
    "saved_model_dir = root_dir / 'models'\n",
    "saved_model_dir = saved_model_dir.resolve()\n",
    "print(f\"Assets Directory: {saved_model_dir}\")\n",
    "if not saved_model_dir.exists():\n",
    "    raise FileNotFoundError('Model directory not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(assets_dir, seq_len=960, batch_size=32):\n",
    "\n",
    "    # train dataset\n",
    "    train_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_train_df.parquet'\n",
    "    train_df_extend = load_df_to_dataset(train_pickle_path_extend).data\n",
    "\n",
    "    # validation dataset\n",
    "    validate_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_validate_df.parquet'\n",
    "    validate_df_extend = load_df_to_dataset(validate_pickle_path_extend).data\n",
    "\n",
    "    # Define the list of features to discard\n",
    "    drop_features_list = ['epoch', 'datetime', 'obj_id', 'traj_id', 'stopped', 'curv', 'abs_ccs']\n",
    "    \n",
    "    columns_to_clean = ['speed_c', 'lon', 'lat']  # Specify columns to clean\n",
    "    cleaned_train_data = clean_outliers_by_quantile(train_df_extend, columns_to_clean, remove_na=False)\n",
    "    cleaned_val_data = clean_outliers_by_quantile(validate_df_extend, columns_to_clean, remove_na=False)\n",
    "    \n",
    "    # Create training and validation datasets\n",
    "    train_dataset_traj = TrajectoryDataset(\n",
    "        cleaned_train_data,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        scaler_method='QuantileTransformer',\n",
    "        filter_less_seq_len = seq_len\n",
    "    )\n",
    "    val_dataset_traj = TrajectoryDataset(\n",
    "        cleaned_val_data,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        scaler_method='QuantileTransformer',\n",
    "        filter_less_seq_len = seq_len\n",
    "    )\n",
    "\n",
    "    # Creating Data Loader\n",
    "    train_dataloader_traj = DataLoader(\n",
    "        train_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    val_dataloader_traj = DataLoader(\n",
    "        val_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    return train_dataloader_traj, val_dataloader_traj, train_dataset_traj.n_features, val_dataset_traj, cleaned_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataloader_traj, val_dataloader_traj, input_dim, val_dataset_traj, cleaned_val_data = load_datasets(assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "learning_rate = 1e-4  # Ensure this is a float, not a tuple\n",
    "lambda_reg = 0.5\n",
    "num_epochs = 20\n",
    "\n",
    "save_model_path = saved_model_dir + '/eae_model_qt_lambda05_960_20e.pth'\n",
    "\n",
    "# Define the model, criterion, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EvidentialTransformerDenoiseAutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    d_model=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "criterion = evidential_regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_losses, val_losses, train_aleatoric_uncertainties, train_epistemic_uncertainties,\n",
    " val_aleatoric_uncertainties, val_epistemic_uncertainties, train_aleatoric_uncertainties_avg, train_epistemic_uncertainties_avg,\n",
    "            val_aleatoric_uncertainties_avg, val_epistemic_uncertainties_avg, latent_representations, recon_error) = train_and_evaluate_local(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader_traj,\n",
    "    val_dataloader=val_dataloader_traj,\n",
    "    num_epochs=num_epochs,\n",
    "    lambda_reg=lambda_reg,\n",
    "    offset=2.5,\n",
    "    device=device,\n",
    "    return_latent=True,\n",
    "    save_model_path=save_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "plot_loss(epochs, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_uncertainty(epochs, train_aleatoric_uncertainties_avg, val_aleatoric_uncertainties_avg, train_epistemic_uncertainties_avg, val_epistemic_uncertainties_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations, val_epistemic_uncertainties, uncertainty_type='epistemic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations, val_aleatoric_uncertainties, uncertainty_type='aleatoric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_aleatoric_uncertainties, val_epistemic_uncertainties, avg_aleatoric_uncertainty, avg_epistemic_uncertainty, latent_representations_eval, recon_error = evaluate_saved_model(\n",
    "    model_class=model, \n",
    "    model_path=save_model_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_dataloader_traj, \n",
    "    lambda_reg=lambda_reg, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
