{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "##> import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "#> import flower\n",
    "import flwr as fl\n",
    "\n",
    "#> import custom libraries\n",
    "from src.load import load_df_to_dataset\n",
    "from src.EAE import EvidentialTransformerDenoiseAutoEncoder, evidential_regression\n",
    "from src.client import train_and_evaluate_local, evaluate_saved_model\n",
    "from src.datasets import TrajectoryDataset\n",
    "from src.plot import plot_loss, plot_tsne_with_uncertainty, plot_uncertainty\n",
    "\n",
    "#> torch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#> Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "#plt.style.use(['science', 'grid', 'notebook'])  # , 'ieee'\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "#%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset catalog\n",
    "assets_dir = Path(\"/data1/aistraj/bin/tvt_assets\").resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=960\n",
    "batch_size=32\n",
    "\n",
    "# train dataset\n",
    "train_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_train_df.parquet'\n",
    "train_df_extend = load_df_to_dataset(train_pickle_path_extend).data\n",
    "\n",
    "# Define the list of features to discard\n",
    "drop_features_list = ['epoch', 'datetime', 'obj_id', 'traj_id', 'stopped', 'curv', 'abs_ccs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers_by_quantile(dataframe, columns_to_clean, iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Clean outliers in the specified columns of a DataFrame using the IQR (Interquartile Range) method.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The DataFrame to clean.\n",
    "    columns_to_clean (list): List of column names to clean for outliers.\n",
    "    iqr_multiplier (float): The multiplier for the IQR to define outlier limits (default is 1.5).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Copy the original DataFrame to avoid modifying it directly\n",
    "    cleaned_data = dataframe.copy()\n",
    "\n",
    "    # Apply IQR filtering for each column\n",
    "    for col in columns_to_clean:\n",
    "        Q1 = cleaned_data[col].quantile(0.25)  # 1st quartile (25th percentile)\n",
    "        Q3 = cleaned_data[col].quantile(0.75)  # 3rd quartile (75th percentile)\n",
    "        IQR = Q3 - Q1  # Interquartile range\n",
    "\n",
    "        # Calculate lower and upper limits\n",
    "        lower_limit = Q1 - iqr_multiplier * IQR\n",
    "        upper_limit = Q3 + iqr_multiplier * IQR\n",
    "\n",
    "        # Print the calculated limits (optional)\n",
    "        print(f\"{col}: lower_limit = {lower_limit}, upper_limit = {upper_limit}\")\n",
    "\n",
    "        # Filter the DataFrame for the current column\n",
    "        cleaned_data = cleaned_data[\n",
    "            (cleaned_data[col] >= lower_limit) & (cleaned_data[col] <= upper_limit)\n",
    "        ]\n",
    "\n",
    "    # Print the number of rows before and after cleaning\n",
    "    print(f\"Total rows before cleaning: {len(dataframe)}\")\n",
    "    print(f\"Total rows after cleaning: {len(cleaned_data)}\")\n",
    "\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_na_rows(dataframe):\n",
    "    \n",
    "    # Total number of rows\n",
    "    total_rows = len(dataframe)\n",
    "\n",
    "    # Check if each row contains any <NA>\n",
    "    na_rows = dataframe.isna().any(axis=1)\n",
    "\n",
    "    # Count the number of rows with <NA>\n",
    "    na_row_count = na_rows.sum()\n",
    "\n",
    "    # Output the results\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Rows containing <NA>: {na_row_count}\")\n",
    "\n",
    "    return total_rows, na_row_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_column_distribution(dataframe, columns, surrounding_range=3, bins=30):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of specified columns in a DataFrame and extract details about the maximum value.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The DataFrame to analyze.\n",
    "    columns (list): List of column names to analyze.\n",
    "    surrounding_range (int): Number of rows to include around the maximum value (default is 3).\n",
    "    bins (int): Number of bins for the histogram (default is 30).\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        print(column)\n",
    "        \n",
    "        # Plot histogram\n",
    "        plt.figure()\n",
    "        dataframe[column].hist(bins=bins, grid=False, edgecolor='black')\n",
    "        \n",
    "        # Find max value and its index\n",
    "        max_index = dataframe[column].idxmax()\n",
    "        max_value = dataframe[column].max()\n",
    "        max_count = (dataframe[column] == max_value).sum()\n",
    "        \n",
    "        print(f\"Max index and value: {max_index}, {max_value}, Count: {max_count}\")\n",
    "        \n",
    "        # Get surrounding rows\n",
    "        index_list = dataframe[column].index.to_list()\n",
    "        max_pos = index_list.index(max_index)\n",
    "        \n",
    "        # Calculate surrounding indices\n",
    "        start_pos = max(0, max_pos - surrounding_range)\n",
    "        end_pos = min(len(index_list) - 1, max_pos + surrounding_range)\n",
    "        \n",
    "        # Extract corresponding rows\n",
    "        result = dataframe[[column]].loc[index_list[start_pos:end_pos + 1]]\n",
    "        \n",
    "        print(result)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without clean outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows, na_row_count = count_na_rows(train_df_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "train_dataset_traj = TrajectoryDataset(\n",
    "    train_df_extend,\n",
    "    seq_len=seq_len,\n",
    "    mode='ae',\n",
    "    drop_features_list=drop_features_list,\n",
    "    scaler=\"MinMaxScaler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_analyze = ['stopped', 'cog_c', 'aad', 'rot_c', 'speed_c', \n",
    "                      'distance_c', 'acc_c', 'cdd', 'dir_ccs', \n",
    "                      'dist_ww', 'dist_ra', 'dist_cl', 'dist_ma', \n",
    "                      'lon', 'lat', 'season', 'part_of_day', \n",
    "                      'month_sin', 'month_cos', 'hour_sin', 'hour_cos']\n",
    "analyze_column_distribution(train_dataset_traj.dataframe, columns_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Outlier and using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['speed_c', 'lon', 'lat']  # Specify columns to clean\n",
    "cleaned_train_data = clean_outliers_by_quantile(train_df_extend, columns_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows_clean, na_row_count_clean = count_na_rows(cleaned_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "clean_train_dataset_traj = TrajectoryDataset(\n",
    "    cleaned_train_data,\n",
    "    seq_len=seq_len,\n",
    "    mode='ae',\n",
    "    drop_features_list=drop_features_list,\n",
    "    scaler=\"MinMaxScaler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_analyze = ['stopped', 'cog_c', 'aad', 'rot_c', 'speed_c', \n",
    "                      'distance_c', 'acc_c', 'cdd', 'dir_ccs', \n",
    "                      'dist_ww', 'dist_ra', 'dist_cl', 'dist_ma', \n",
    "                      'lon', 'lat', 'season', 'part_of_day', \n",
    "                      'month_sin', 'month_cos', 'hour_sin', 'hour_cos']\n",
    "analyze_column_distribution(clean_train_dataset_traj.dataframe, columns_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Outlier and using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "clean_train_dataset_traj_s = TrajectoryDataset(\n",
    "    cleaned_train_data,\n",
    "    seq_len=seq_len,\n",
    "    mode='ae',\n",
    "    drop_features_list=drop_features_list,\n",
    "    scaler=\"StandardScaler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_column_distribution(clean_train_dataset_traj_s.dataframe, columns_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Outlier and using RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "clean_train_dataset_traj_r = TrajectoryDataset(\n",
    "    cleaned_train_data,\n",
    "    seq_len=seq_len,\n",
    "    mode='ae',\n",
    "    drop_features_list=drop_features_list,\n",
    "    scaler=\"RobustScaler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_column_distribution(clean_train_dataset_traj_r.dataframe, columns_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "clean_train_dataset_traj_r = TrajectoryDataset(\n",
    "    cleaned_train_data,\n",
    "    seq_len=seq_len,\n",
    "    mode='ae',\n",
    "    drop_features_list=drop_features_list,\n",
    "    scaler=\"QuantileTransformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_column_distribution(clean_train_dataset_traj_r.dataframe, columns_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
