{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "##> import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "from typing import OrderedDict\n",
    "\n",
    "\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "#> import flower\n",
    "import flwr as fl\n",
    "from flwr.common import Context\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner, DirichletPartitioner, NaturalIdPartitioner\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "#> import custom libraries\n",
    "from src.load import load_df_to_dataset\n",
    "from src.EAE import EvidentialTransformerDenoiseAutoEncoder, evidential_regression\n",
    "from src.client import EAEClient, evaluate_saved_model\n",
    "from src.datasets import TrajectoryDataset, clean_outliers_by_quantile\n",
    "from src.plot import plot_tsne_with_uncertainty, plot_uncertainty\n",
    "\n",
    "#> torch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#> Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "plt.style.use(['science', 'grid', 'notebook', 'ieee'])  # , 'ieee'\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the dataset catalog\n",
    "assets_dir = root_dir.parents[3] / 'aistraj' / 'bin'/ 'tvt_assets'\n",
    "assets_dir = assets_dir.resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "    \n",
    "saved_model_dir = root_dir / 'models'\n",
    "saved_model_dir = saved_model_dir.resolve()\n",
    "print(f\"Assets Directory: {saved_model_dir}\")\n",
    "if not saved_model_dir.exists():\n",
    "    raise FileNotFoundError('Model directory not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # setup_environment()\n",
    "if multiprocessing.get_start_method(allow_none=True) != \"spawn\":\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "\n",
    "# Define the dataset catalog\n",
    "assets_dir = Path(\"/data1/aistraj/bin/tvt_assets\").resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "\n",
    "# Set the working directory to the 'src' directory, which contains only the code.\n",
    "code_dir = root_dir / 'src'\n",
    "code_dir = code_dir.resolve()\n",
    "print(f\"Code Directory: {code_dir}\")\n",
    "if not code_dir.exists():\n",
    "    raise FileNotFoundError('Code directory not found')\n",
    "\n",
    "excludes = [\"data\", \"*.pyc\", \"__pycache__\"\n",
    "]\n",
    "\n",
    "ray_init_args = {\n",
    "    \"runtime_env\": {\n",
    "        #\"working_dir\": str(code_dir),\n",
    "        \"py_modules\": [str(code_dir)],\n",
    "        \"excludes\": [str(code_dir / file) for file in excludes]\n",
    "    },\n",
    "    \"include_dashboard\": False,\n",
    "    #\"num_cpus\": 4,\n",
    "    # \"local_mode\": True\n",
    "}\n",
    "\n",
    "num_clients = 4\n",
    "\n",
    "config = {\n",
    "    \"lambda_reg\": 0.5,     \n",
    "    \"num_epochs\": 1,        \n",
    "    \"offset\": 2.5,       \n",
    "}\n",
    "\n",
    "global_model_save_path = '/data1/sgao/repos/CogSigma/oNSA/models/feae_model_global_lambda05_random_960_20e.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metrics(metrics):\n",
    "    \"\"\"Aggregate evaluation metrics from multiple clients.\"\"\"\n",
    "    if not metrics:\n",
    "        return {}\n",
    "    \n",
    "    # Print evaluation metrics for each client for debugging purposes\n",
    "    for idx, m in enumerate(metrics):\n",
    "        print(f\"Client {idx}: {m}\")\n",
    "    \n",
    "    # Extract the dictionary part returned by all clients and verify its structure\n",
    "    metrics_dicts = []\n",
    "    for m in metrics:\n",
    "        if isinstance(m, tuple) and len(m) == 2 and isinstance(m[1], dict):\n",
    "            metrics_dicts.append(m[1])\n",
    "        else:\n",
    "            print(f\"Unexpected metrics format: {m}\")  \n",
    "\n",
    "    if not metrics_dicts:\n",
    "        print(\"No valid metrics to aggregate.\")\n",
    "        return {}\n",
    "\n",
    "    # Aggregate metrics_dicts that match the format of the\n",
    "    aggregated = {}\n",
    "    for key in metrics_dicts[0].keys():\n",
    "        aggregated[key] = sum(m[key] for m in metrics_dicts) / len(metrics_dicts)\n",
    "    print(\"All metrics received from clients:\", metrics)\n",
    "    return aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_eval(assets_dir, seq_len=960, batch_size=32):\n",
    "\n",
    "    # validation dataset\n",
    "    validate_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_validate_df.parquet'\n",
    "    validate_df_extend = load_df_to_dataset(validate_pickle_path_extend).data\n",
    "\n",
    "    # Define the list of features to discard\n",
    "    drop_features_list = ['epoch', 'datetime', 'obj_id', 'traj_id', 'stopped', 'curv', 'abs_ccs']\n",
    "    columns_to_clean = ['speed_c', 'lon', 'lat']  # Specify columns to clean\n",
    "    \n",
    "    validate_df_extend = clean_outliers_by_quantile(validate_df_extend, columns_to_clean, remove_na=False)\n",
    "    \n",
    "    val_dataset_traj = TrajectoryDataset(\n",
    "        validate_df_extend,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        scaler_method='QuantileTransformer',\n",
    "        filter_less_seq_len = seq_len\n",
    "    )\n",
    "\n",
    "    val_dataloader_traj = DataLoader(\n",
    "        val_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    return val_dataloader_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFederatedDataset(FederatedDataset):\n",
    "    def _prepare_dataset(self) -> None:\n",
    "        \"\"\"Override the original method and use the local dataset directly to avoid loading from the Hugging Face Hub.\"\"\"\n",
    "        self._dataset_prepared = True\n",
    "\n",
    "def load_datasets_fl_with_partitioner(\n",
    "    assets_dir,\n",
    "    num_clients,\n",
    "    seq_len=960,\n",
    "    batch_size=32,\n",
    "    partitioner_type=\"iid\",\n",
    "    alpha=0.5, # Smoothing parameters for the Dirichlet distribution,\n",
    "    partition_column=None,\n",
    "    **partitioner_kwargs,\n",
    "):\n",
    "    # Load local training and validation datasets\n",
    "    train_pickle_path_extend = assets_dir / \"extended\" / \"cleaned_extended_train_df.parquet\"\n",
    "    train_df_extend = load_df_to_dataset(train_pickle_path_extend).data\n",
    "\n",
    "    validate_pickle_path_extend = assets_dir / \"extended\" / \"cleaned_extended_validate_df.parquet\"\n",
    "    validate_df_extend = load_df_to_dataset(validate_pickle_path_extend).data\n",
    "\n",
    "    columns_to_clean = ['speed_c', 'lon', 'lat']  # Specify columns to clean\n",
    "    train_df_extend = clean_outliers_by_quantile(train_df_extend, columns_to_clean, remove_na=False)\n",
    "    validate_df_extend = clean_outliers_by_quantile(validate_df_extend, columns_to_clean, remove_na=False)\n",
    "    \n",
    "    print(\"After correction:\")\n",
    "    print(\"Unique 'season' values in training dataset:\", train_df_extend['season'].unique())\n",
    "    print(\"Unique 'season' values in validation dataset:\", validate_df_extend['season'].unique())\n",
    "    \n",
    "    #  Convert pandas.DataFrame to datasets.\n",
    "    train_dataset = Dataset.from_pandas(train_df_extend, preserve_index=False)\n",
    "    val_dataset = Dataset.from_pandas(validate_df_extend, preserve_index=False)\n",
    "\n",
    "    # Choose the partitioner\n",
    "    if partitioner_type == \"iid\":\n",
    "        partitioner_class = IidPartitioner(num_partitions=num_clients, **partitioner_kwargs)\n",
    "        \n",
    "    elif partitioner_type == \"naturalidpartitioner\":\n",
    "        if partition_column is None:\n",
    "            raise ValueError(\"partition_column must be specified when using DirichletPartitioner.\")\n",
    "        partitioner_class = NaturalIdPartitioner(\n",
    "        #num_partitions=num_clients,\n",
    "        partition_by=partition_column,\n",
    "        **partitioner_kwargs,\n",
    "        )\n",
    "    elif partitioner_type == \"dirichlet\":\n",
    "        if partition_column is None:\n",
    "            raise ValueError(\"partition_column must be specified when using DirichletPartitioner.\")\n",
    "        partitioner_class = DirichletPartitioner(\n",
    "            num_partitions=num_clients,\n",
    "            alpha=alpha,\n",
    "            partition_by=partition_column,\n",
    "            **partitioner_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown partitioner type: {partitioner_type}\")\n",
    "\n",
    "    # Initialize custom FederatedDataset\n",
    "    fds_train = CustomFederatedDataset(\n",
    "        dataset=\"train\",\n",
    "        partitioners={\"train\": partitioner_class},\n",
    "    )\n",
    "    fds_val = CustomFederatedDataset(\n",
    "        dataset=\"val\",\n",
    "        partitioners={\"val\": partitioner_class},\n",
    "    )\n",
    "\n",
    "    # Manually assign locally loaded datasets.Dataset\n",
    "    fds_train._dataset = {\"train\": train_dataset}\n",
    "    fds_val._dataset = {\"val\": val_dataset}\n",
    "\n",
    "    # Define features to be discarded\n",
    "    drop_features_list = [\"epoch\", \"datetime\", \"obj_id\", \"traj_id\", \"stopped\", \"curv\", \"abs_ccs\"]\n",
    "\n",
    "    # Create per-client data loaders\n",
    "    train_dataloaders, val_dataloaders = [], []\n",
    "    n_features = None\n",
    "\n",
    "    for client_id in range(num_clients):\n",
    "        train_partition = fds_train.load_partition(client_id, split=\"train\").to_pandas()\n",
    "        val_partition = fds_val.load_partition(client_id, split=\"val\").to_pandas()\n",
    "        train_seasons = set(train_partition['season'].unique())\n",
    "        print(f\"Client {client_id + 1} season values: {train_seasons}\")\n",
    "\n",
    "        train_dataset_traj = TrajectoryDataset(\n",
    "            train_partition,\n",
    "            seq_len=seq_len,\n",
    "            mode=\"ae\",\n",
    "            drop_features_list=drop_features_list,\n",
    "            scaler_method='QuantileTransformer',\n",
    "            filter_less_seq_len = seq_len\n",
    "        )\n",
    "        val_dataset_traj = TrajectoryDataset(\n",
    "            val_partition,\n",
    "            seq_len=seq_len,\n",
    "            mode=\"ae\",\n",
    "            drop_features_list=drop_features_list,\n",
    "            scaler_method='QuantileTransformer',\n",
    "            filter_less_seq_len = seq_len\n",
    "        )\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset_traj,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset_traj,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        train_dataloaders.append(train_dataloader)\n",
    "        val_dataloaders.append(val_dataloader)\n",
    "\n",
    "        if n_features is None:\n",
    "            n_features = train_dataset_traj.n_features\n",
    "\n",
    "    return train_dataloaders, val_dataloaders, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders, val_dataloaders, input_dim = load_datasets_fl_with_partitioner(\n",
    "    assets_dir=assets_dir,\n",
    "    num_clients=num_clients,\n",
    "    seq_len=960,\n",
    "    batch_size=32,\n",
    "    partitioner_type=\"iid\", # dirichlet, naturalidpartitioner, iid\n",
    "    #alpha=0.5,\n",
    "    # partition_column='season'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for client_id, dataloader in enumerate(train_dataloaders):\n",
    "    df = dataloader.dataset.dataframe\n",
    "    unique_seasons = df['season'].unique()\n",
    "    print(f\"Client {client_id + 1} season values: {unique_seasons}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seasons = ['Spring', 'Summer', 'Autumn', 'Winter']\n",
    "\n",
    "client_season_distributions = []\n",
    "\n",
    "for client_id, dataloader in enumerate(train_dataloaders):\n",
    "    season_counts = []\n",
    "    for sample in dataloader.dataset:\n",
    "        inputs = sample['inputs']  # [seq_len, n_features]\n",
    "        input_mask = sample['input_masks']  # [seq_len]\n",
    "\n",
    "        valid_season_values = inputs[input_mask.bool(), -1].numpy()  \n",
    "        season_counts.extend(valid_season_values.tolist())  \n",
    "    \n",
    "    season_distribution = pd.Series(season_counts).value_counts().sort_index()\n",
    "    season_distribution = season_distribution.reindex(range(4), fill_value=0)  \n",
    "\n",
    "    season_distribution.index = all_seasons\n",
    "    client_season_distributions.append(season_distribution)\n",
    "\n",
    "season_distribution_df = pd.DataFrame(client_season_distributions).T\n",
    "season_distribution_df.columns = [f'Client {i+1}' for i in range(len(client_season_distributions))]\n",
    "print(season_distribution_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_distribution_df.T.plot(kind='bar', stacked=True, figsize=(12, 8), width=0.7, cmap='tab20')\n",
    "plt.title(\"Season Distribution Across Clients\", fontsize=25)\n",
    "#plt.xlabel(\"Clients\", fontsize=14)\n",
    "#plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xticks(fontsize=25, rotation=0)  # Make the x-tick labels (Client1, Client2, etc.) larger\n",
    "plt.yticks(fontsize=25)\n",
    "plt.legend(title=\"Season\", fontsize=18)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(client_id: int):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Select the corresponding client data from the list\n",
    "    train_dataloader = train_dataloaders[client_id]\n",
    "    val_dataloader = val_dataloaders[client_id]\n",
    "\n",
    "    # Initialization Model\n",
    "    model = EvidentialTransformerDenoiseAutoEncoder(\n",
    "        input_dim=input_dim,\n",
    "        d_model=8,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        dim_feedforward=32,\n",
    "        max_seq_length=960,\n",
    "        dropout_rate=0.1,\n",
    "    ).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = evidential_regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Returns the client instance\n",
    "    return EAEClient(\n",
    "        cid=client_id,\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        save_model_path=None,\n",
    "    ).to_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelFedAvg(fl.server.strategy.FedAvg):\n",
    "    def __init__(self, save_path, model_architecture, device, num_rounds, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_path = save_path\n",
    "        self.model_architecture = model_architecture\n",
    "        self.device = device\n",
    "        self.num_rounds = num_rounds  \n",
    "\n",
    "    def aggregate_fit(self, rnd, results, failures):\n",
    "        # Calling the aggregation methods of the parent class\n",
    "        aggregated_result = super().aggregate_fit(rnd, results, failures)\n",
    "        if aggregated_result is not None:\n",
    "            parameters_aggregated, metrics_aggregated = aggregated_result\n",
    "            # Saving the global model in the last round\n",
    "            if rnd == self.num_rounds:\n",
    "                print(f\"Saving global model at round {rnd}\")\n",
    "                self.save_model(parameters_aggregated)\n",
    "        return aggregated_result\n",
    "\n",
    "    def save_model(self, parameters):\n",
    "        # Convert parameters to NumPy format\n",
    "        params_ndarrays = fl.common.parameters_to_ndarrays(parameters)\n",
    "\n",
    "        # Initialization Model\n",
    "        model = self.model_architecture().to(self.device)\n",
    "\n",
    "        # Setting Model Parameters\n",
    "        params_dict = zip(model.state_dict().keys(), params_ndarrays)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        # Saving Models\n",
    "        torch.save(model.state_dict(), self.save_path)\n",
    "        print(f\"Global model saved to {self.save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 20\n",
    "\n",
    "strategy = SaveModelFedAvg(\n",
    "    save_path=global_model_save_path,\n",
    "    model_architecture=lambda: EvidentialTransformerDenoiseAutoEncoder(\n",
    "        input_dim=input_dim,\n",
    "        d_model=8,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        dim_feedforward=32,\n",
    "        max_seq_length=960,\n",
    "        dropout_rate=0.1,\n",
    "    ),\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    num_rounds=num_rounds,  \n",
    "    fraction_fit=1,\n",
    "    fraction_evaluate=1,\n",
    "    min_fit_clients=num_clients,\n",
    "    min_evaluate_clients=num_clients,\n",
    "    min_available_clients=num_clients,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=lambda cid: client_fn(int(cid)),\n",
    "    num_clients=num_clients,\n",
    "    config=fl.server.ServerConfig(num_rounds=num_rounds),\n",
    "    strategy=strategy,\n",
    "    client_resources={\"num_cpus\": 1.0, \"num_gpus\": 0.25},\n",
    "    ray_init_args=ray_init_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = EvidentialTransformerDenoiseAutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    d_model=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "val_dataloader_traj = load_datasets_eval(assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_aleatoric_uncertainties, val_epistemic_uncertainties, avg_aleatoric_uncertainty, avg_epistemic_uncertainty, latent_representations_eval, recon_error = evaluate_saved_model(\n",
    "    model_class=global_model, \n",
    "    model_path=global_model_save_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_dataloader_traj, \n",
    "    lambda_reg=1, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations_eval, val_aleatoric_uncertainties, uncertainty_type='aleatoric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations_eval, val_epistemic_uncertainties, uncertainty_type='epistemic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
