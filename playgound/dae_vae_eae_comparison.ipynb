{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "##> import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "from typing import OrderedDict\n",
    "\n",
    "\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "#> import flower\n",
    "import flwr as fl\n",
    "from flwr.common import Context\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner, DirichletPartitioner\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "#> import custom libraries\n",
    "from src.load import load_df_to_dataset\n",
    "from src.EAE import EvidentialTransformerDenoiseAutoEncoder, evidential_regression\n",
    "from src.client import train_and_evaluate_local, evaluate_saved_model\n",
    "from src.datasets import TrajectoryDataset, clean_outliers_by_quantile, generate_ood_data\n",
    "from src.plot import plot_tsne_with_uncertainty, visualize_mean_features\n",
    "\n",
    "#> torch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, roc_curve\n",
    "\n",
    "#> Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "# plt.style.use(['science', 'grid', 'notebook', 'ieee'])  # , 'ieee'\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the dataset catalog\n",
    "assets_dir = root_dir.parents[3] / 'aistraj' / 'bin'/ 'tvt_assets'\n",
    "assets_dir = assets_dir.resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "    \n",
    "saved_model_dir = root_dir / 'models'\n",
    "saved_model_dir = saved_model_dir.resolve()\n",
    "print(f\"Assets Directory: {saved_model_dir}\")\n",
    "if not saved_model_dir.exists():\n",
    "    raise FileNotFoundError('Model directory not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # setup_environment()\n",
    "if multiprocessing.get_start_method(allow_none=True) != \"spawn\":\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "\n",
    "# Define the dataset catalog\n",
    "assets_dir = Path(\"/data1/aistraj/bin/tvt_assets\").resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "\n",
    "# Set the working directory to the 'src' directory, which contains only the code.\n",
    "code_dir = root_dir / 'src'\n",
    "code_dir = code_dir.resolve()\n",
    "print(f\"Code Directory: {code_dir}\")\n",
    "if not code_dir.exists():\n",
    "    raise FileNotFoundError('Code directory not found')\n",
    "\n",
    "excludes = [\"data\", \"*.pyc\", \"__pycache__\"\n",
    "]\n",
    "\n",
    "ray_init_args = {\n",
    "    \"runtime_env\": {\n",
    "        #\"working_dir\": str(code_dir),\n",
    "        \"py_modules\": [str(code_dir)],\n",
    "        \"excludes\": [str(code_dir / file) for file in excludes]\n",
    "    },\n",
    "    \"include_dashboard\": False,\n",
    "    #\"num_cpus\": 4,\n",
    "    # \"local_mode\": True\n",
    "}\n",
    "\n",
    "num_clients = 4\n",
    "\n",
    "# config = {\n",
    "#     \"lambda_reg\": 1,     \n",
    "#     \"num_epochs\": 1,        \n",
    "#     \"offset\": 2.5,       \n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_model_save_path = '/data1/sgao/repos/CogSigma/oNSA/models/dae_model_qt_960.pth'\n",
    "vae_model_save_path = '/data1/sgao/repos/CogSigma/oNSA/models/vae_model_qt_960_20.pth'\n",
    "eae_model_save_path = '/data1/sgao/repos/CogSigma/oNSA/models/eae_model_qt_lambda05_960.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_eval(assets_dir, seq_len=960, batch_size=32):\n",
    "    \n",
    "     # train dataset\n",
    "    train_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_train_df.parquet'\n",
    "    train_df_extend = load_df_to_dataset(train_pickle_path_extend).data\n",
    "\n",
    "    # validation dataset\n",
    "    validate_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_validate_df.parquet'\n",
    "    validate_df_extend = load_df_to_dataset(validate_pickle_path_extend).data\n",
    "    ood_df = generate_ood_data(validate_df_extend, ood_mean=1, ood_std=1)\n",
    "    #print (ood_df.shape)\n",
    "\n",
    "    # Define the list of features to discard\n",
    "    drop_features_list = ['epoch', 'datetime', 'obj_id', 'traj_id', 'stopped', 'curv', 'abs_ccs']\n",
    "    columns_to_clean = ['speed_c', 'lon', 'lat']  # Specify columns to clean\n",
    "    \n",
    "    cleaned_train_data = clean_outliers_by_quantile(train_df_extend, columns_to_clean, remove_na=False)\n",
    "    cleaned_val_data = clean_outliers_by_quantile(validate_df_extend, columns_to_clean, remove_na=False)\n",
    "    \n",
    "    df_extend = pd.concat([cleaned_train_data, cleaned_val_data])\n",
    "    df_extend = df_extend.sort_index()\n",
    "    \n",
    "    val_dataset_traj = TrajectoryDataset(\n",
    "        cleaned_val_data,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        scaler_method='QuantileTransformer',\n",
    "        filter_less_seq_len=6\n",
    "        #categorical_features=['season']\n",
    "    )\n",
    "    \n",
    "    val_ood_dataset_traj = TrajectoryDataset(\n",
    "        ood_df,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        filter_percent = None,\n",
    "        scaler = None,\n",
    "        filter_less_seq_len = None,\n",
    "        scaler_method = 'No_Scaler'\n",
    "    )    \n",
    "    #print (val_ood_dataset_traj.inputs)\n",
    "\n",
    "    val_dataloader_traj = DataLoader(\n",
    "        val_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_ood_dataloader_traj = DataLoader(\n",
    "        val_ood_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    return val_dataloader_traj, val_ood_dataloader_traj, val_dataset_traj.n_features, val_dataset_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "val_dataloader_traj, val_ood_dataloader_traj, input_dim, dataset_traj = load_datasets_eval(assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ood_dataloader_traj.dataset.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader_traj.dataset.inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and utlis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "        nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class TransformerAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, latent_dim, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, dropout_rate, output_dim=None):\n",
    "        super(TransformerAutoEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        if output_dim:\n",
    "            self.output_dim = output_dim\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = nn.Parameter(self._generate_positional_encoding(self.max_seq_length, d_model), requires_grad=False)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # Fully connected layers for encoding inputs and decoding outputs\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        self.output_fc = nn.Linear(d_model, output_dim or input_dim)   # Output 4x dimensions in order to separate mu, v, alpha, beta\n",
    "\n",
    "        self.bottleneck = nn.Linear(self.max_seq_length*d_model, 10)  # Optional, can skip if not needed\n",
    "        self.reconstruct = nn.Linear(10, self.max_seq_length*d_model)  # Project back to the original space\n",
    "        self.flatten = nn.Flatten(start_dim=1,end_dim=2)#\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(self.max_seq_length, d_model))\n",
    "        # Apply weight initialization\n",
    "        self.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, src, padding_mask=None, return_latent=False, noise_factor=0.05):\n",
    "        # add noise\n",
    "        noise = torch.randn_like(src) * noise_factor\n",
    "        noisy_src = src + noise\n",
    "        noisy_src = self.input_fc(noisy_src)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # add positional embedding\n",
    "        noisy_src += self.positional_encoding[:, :noisy_src.size(1), :]\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            # padding_mask:(batch_size, seq_len, input_dim)\n",
    "            padding_mask_timestep = padding_mask.any(dim=-1)  # [batch_size, seq_len]\n",
    "            mask_expanded = padding_mask_timestep.unsqueeze(-1).expand_as(noisy_src).bool()\n",
    "            # mask\n",
    "            noisy_src = torch.where(mask_expanded, noisy_src, torch.tensor(0.0, device=noisy_src.device))\n",
    "\n",
    "        # encode\n",
    "        encoded_memory = self.transformer_encoder(noisy_src)  \n",
    "        \n",
    "        # decode\n",
    "        decoded_output = self.transformer_decoder(encoded_memory, encoded_memory)\n",
    "        decoded_output = self.output_fc(decoded_output)  # Shape: (batch_size, seq_length, output_dim * 4)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            mask_expanded = padding_mask_timestep.unsqueeze(-1).expand(-1, -1, decoded_output.size(-1)).float()\n",
    "            decoded_output = decoded_output * mask_expanded\n",
    "\n",
    "        return decoded_output, encoded_memory\n",
    "\n",
    "\n",
    "    def _generate_positional_encoding(self, length, d_model):\n",
    "        position = torch.arange(length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pos_encoding = torch.zeros(length, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding.unsqueeze(0)  # add batch dimension\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class TransformerVariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, latent_dim, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, dropout_rate, output_dim=None):\n",
    "        super(TransformerVariationalAutoEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        if output_dim:\n",
    "            self.output_dim = output_dim\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = nn.Parameter(self._generate_positional_encoding(self.max_seq_length, d_model), requires_grad=False)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.encode_mean = nn.Linear(self.max_seq_length*d_model, latent_dim)\n",
    "        self.encode_logvar = nn.Linear(self.max_seq_length*d_model, latent_dim)\n",
    "        self.decode_adapter = nn.Linear(latent_dim, self.max_seq_length*d_model)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # Fully connected layers for encoding inputs and decoding outputs\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        self.output_fc = nn.Linear(d_model, output_dim or input_dim)   # Output 4x dimensions in order to separate mu, v, alpha, beta\n",
    "\n",
    "        self.bottleneck = nn.Linear(self.max_seq_length*d_model, 10)  # Optional, can skip if not needed\n",
    "        self.reconstruct = nn.Linear(10, self.max_seq_length*d_model)  # Project back to the original space\n",
    "        self.flatten = nn.Flatten(start_dim=1,end_dim=2)#\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(self.max_seq_length, d_model))\n",
    "        # Apply weight initialization\n",
    "        self.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, src, padding_mask=None, return_latent=False, noise_factor=0.05):\n",
    "        # add noise\n",
    "        noise = torch.randn_like(src) * noise_factor\n",
    "        noisy_src = src + noise\n",
    "        noisy_src = self.input_fc(noisy_src)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # add positional embedding\n",
    "        noisy_src += self.positional_encoding[:, :noisy_src.size(1), :]\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            # padding_mask:(batch_size, seq_len, input_dim)\n",
    "            padding_mask_timestep = padding_mask.any(dim=-1)  # [batch_size, seq_len]\n",
    "            mask_expanded = padding_mask_timestep.unsqueeze(-1).expand_as(noisy_src).bool()\n",
    "            # mask\n",
    "            noisy_src = torch.where(mask_expanded, noisy_src, torch.tensor(0.0, device=noisy_src.device))\n",
    "\n",
    "        # encode\n",
    "        encoded_memory = self.transformer_encoder(noisy_src)  \n",
    "        \n",
    "        encoded_output =  encoded_memory.view(encoded_memory.size(0), -1)\n",
    "        mu = self.encode_mean(encoded_output)\n",
    "        log_var = self.encode_logvar(encoded_output)\n",
    "        # reparameter\n",
    "        z, _ = self.reparameterize(mu, log_var)\n",
    "        z = self.decode_adapter(mu).view_as(encoded_memory)\n",
    "        \n",
    "        # decode\n",
    "        decoded_output = self.transformer_decoder(z, z)\n",
    "        decoded_output = self.output_fc(decoded_output)  # Shape: (batch_size, seq_length, output_dim * 4)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            mask_expanded = padding_mask_timestep.unsqueeze(-1).expand(-1, -1, decoded_output.size(-1)).float()\n",
    "            decoded_output = decoded_output * mask_expanded\n",
    "\n",
    "        return decoded_output, mu, log_var\n",
    "\n",
    "\n",
    "    def _generate_positional_encoding(self, length, d_model):\n",
    "        position = torch.arange(length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pos_encoding = torch.zeros(length, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding.unsqueeze(0)  # add batch dimension\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "\n",
    "        std = torch.exp(0.5 * log_var)  # standard deviation\n",
    "        epsilon = torch.randn_like(std)  # random noise\n",
    "        z = mu + epsilon * std  # reparameterize\n",
    "        return z, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_reconstruction_loss(original, reconstructed, mask, offset=0, batch_mean=True):\n",
    "\n",
    "    # Time-step-by-time-step calculation error (MSE)\n",
    "    error = F.mse_loss(reconstructed, original, reduction=\"none\")  # (batch_size, seq_len, feature_dim)\n",
    "    error = error.mean(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "    # mask\n",
    "    masked_error = error * mask  # v\n",
    "\n",
    "    # Averaging over effective time steps\n",
    "    loss = masked_error.sum(dim=1) / mask.sum(dim=1)  \n",
    "    if batch_mean == True:\n",
    "        loss = loss.mean()  \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    loss = loss + offset\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path, device='cpu'):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded from {path}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function for DAE and VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dae_vae(\n",
    "    model_class,\n",
    "    model_path,\n",
    "    device,\n",
    "    test_dataloader,\n",
    "    input_dim=20,\n",
    "    d_model=8,\n",
    "    latent_dim=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1,\n",
    "    criterion=masked_reconstruction_loss\n",
    "):\n",
    "\n",
    "    # Initialization Model\n",
    "    model = model_class(\n",
    "        input_dim=input_dim,\n",
    "        d_model=d_model,\n",
    "        latent_dim=latent_dim,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # Load trained model parameters\n",
    "    load_model(model, model_path, device=device)\n",
    "    model.eval()\n",
    "\n",
    "    latents = []\n",
    "    latents_std = []\n",
    "    recon_errors = []\n",
    "    all_inputs = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            batch_data = batch['inputs'].to(device)\n",
    "            masks = batch.get('input_masks', None)\n",
    "            if masks is not None:\n",
    "                masks = masks.to(device)\n",
    "                if masks.dim() == 2:\n",
    "                    masks_expend = masks.unsqueeze(-1).expand_as(batch_data)\n",
    "            else:\n",
    "                masks_expend = None\n",
    "\n",
    "            # forward propagation\n",
    "            output = model(batch_data, masks_expend)\n",
    "            \n",
    "            # Determine whether it is AE or VAE based on the number of outputs\n",
    "            if len(output) == 2:\n",
    "                # AE: output = (recon_data, latent)\n",
    "                recon_data, latent = output\n",
    "                mu = latent\n",
    "                std = None\n",
    "            elif len(output) == 3:\n",
    "                # VAE: output = (recon_data, mu, log_var)\n",
    "                recon_data, mu, log_var = output\n",
    "                if hasattr(model, 'reparameterize'):\n",
    "                    _, std = model.reparameterize(mu, log_var)\n",
    "                else:\n",
    "                    raise AttributeError(\"The model does not have a reparameterize method, but returned mu and log_var.\")\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected number of outputs from the model forward pass. Expected 2 for AE, or 3 for VAE.\")\n",
    "\n",
    "            # Compute the reconstruction error\n",
    "            loss = criterion(batch_data, recon_data, masks, batch_mean=False)\n",
    "            recon_errors.extend(loss.cpu().numpy())\n",
    "\n",
    "            # Collection of latent expressions and standard deviations\n",
    "            latents.append(mu.cpu().numpy())\n",
    "            if std is not None:\n",
    "                latents_std.append(std.cpu().numpy())\n",
    "\n",
    "            # Collecting inputs and outputs\n",
    "            all_inputs.append(batch_data.cpu().numpy())\n",
    "            all_outputs.append(recon_data.cpu().numpy())\n",
    "\n",
    "    latents = np.vstack(latents)\n",
    "    if len(latents_std) > 0:\n",
    "        latents_std = np.vstack(latents_std)\n",
    "    else:\n",
    "        latents_std = None\n",
    "\n",
    "    inputs = np.vstack(all_inputs)\n",
    "    outputs = np.vstack(all_outputs)\n",
    "\n",
    "    return latents, latents_std, recon_errors, inputs, outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function for EAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_eae(\n",
    "    model_class,\n",
    "    model_path,\n",
    "    criterion,\n",
    "    val_dataloader,\n",
    "    lambda_reg,\n",
    "    offset,\n",
    "    device,\n",
    "    return_latent=False,\n",
    "    input_dim=20,\n",
    "    d_model=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    "):\n",
    "\n",
    "    # Initialization Model\n",
    "    model = model_class(\n",
    "        input_dim=input_dim,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # Load trained model parameters\n",
    "    load_model(model, model_path, device=device)\n",
    "    model.eval()\n",
    "\n",
    "    total_val_loss = 0.0\n",
    "    total_samples = 0\n",
    "    recon_error = []\n",
    "    latent_representations = []\n",
    "    all_inputs = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_dataloader):\n",
    "            features = batch['inputs'].to(device)\n",
    "            masks = batch.get('input_masks', None)\n",
    "            if masks is not None:\n",
    "                masks = masks.to(device)\n",
    "                if masks.dim() == 2:\n",
    "                    masks = masks.unsqueeze(-1).expand_as(features)\n",
    "\n",
    "            if return_latent:\n",
    "                mu, v, alpha, beta, latent = model(features, return_latent=True)\n",
    "                latent_representations.append(latent.cpu())\n",
    "            else:\n",
    "                mu, v, alpha, beta = model(features)\n",
    "\n",
    "            per_sample_reconstruction_error = masked_reconstruction_loss(features, mu, masks[:,:,0] if masks is not None else None, batch_mean=False)\n",
    "            recon_error.extend(per_sample_reconstruction_error.cpu().numpy())\n",
    "\n",
    "            # Calculate the total validation loss (mu can be used within criterion)\n",
    "            val_loss = criterion((mu, v, alpha, beta), features, lambda_reg, offset, mask=masks, recon_error=per_sample_reconstruction_error)\n",
    "            batch_size = features.size(0)\n",
    "            total_val_loss += val_loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            # Collection of input and output data for post-mortem analysis and visualization\n",
    "            all_inputs.append(features.cpu().numpy())\n",
    "            all_outputs.append(mu.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / total_samples\n",
    "\n",
    "    # Concatenating cumulative input and output data\n",
    "    inputs = np.vstack(all_inputs)\n",
    "    outputs = np.vstack(all_outputs)\n",
    "\n",
    "    if return_latent:\n",
    "        latent_representations = torch.cat(latent_representations, dim=0)\n",
    "        return avg_val_loss, recon_error, inputs, outputs, latent_representations\n",
    "    else:\n",
    "        return avg_val_loss, recon_error, inputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ood_metrics(id_scores, ood_scores, threshold_method='percentile', percentile=95, k=1.0, reduce_method='mean'):\n",
    "    # 1. Ensure input data is a NumPy array\n",
    "    id_scores = np.array(id_scores)\n",
    "    ood_scores = np.array(ood_scores)\n",
    "    \n",
    "    # 2. Dimensionality reduction if the input is 2D (batch_size, latent_dim)\n",
    "    if id_scores.ndim > 1:\n",
    "        if reduce_method == 'mean':\n",
    "            id_scores = np.mean(id_scores, axis=1)\n",
    "            ood_scores = np.mean(ood_scores, axis=1)\n",
    "        elif reduce_method == 'max':\n",
    "            id_scores = np.max(id_scores, axis=1)\n",
    "            ood_scores = np.max(ood_scores, axis=1)\n",
    "        elif reduce_method == 'l2':\n",
    "            id_scores = np.linalg.norm(id_scores, axis=1)\n",
    "            ood_scores = np.linalg.norm(ood_scores, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid reduce_method. Available options are 'mean', 'max', 'l2'.\")\n",
    "\n",
    "    # 3. Calculate the threshold\n",
    "    if threshold_method == 'percentile':\n",
    "        threshold = np.percentile(id_scores, percentile)\n",
    "    elif threshold_method == 'mean_std':\n",
    "        threshold = np.mean(id_scores) + k * np.std(id_scores)\n",
    "    else:\n",
    "        raise ValueError(\"threshold_method must be 'percentile' or 'mean_std'\")\n",
    "    \n",
    "    # 4. Concatenate ID and OOD scores\n",
    "    all_scores = np.concatenate([id_scores, ood_scores], axis=0)\n",
    "    \n",
    "    # 5. Create labels (ID is 0, OOD is 1)\n",
    "    labels_id = np.zeros(len(id_scores))  # ID labels\n",
    "    labels_ood = np.ones(len(ood_scores)) # OOD labels\n",
    "    all_labels = np.concatenate([labels_id, labels_ood], axis=0)\n",
    "    \n",
    "    # 6. Generate predictions based on threshold\n",
    "    predictions = (all_scores > threshold).astype(int)\n",
    "    \n",
    "    # 7. Calculate metrics\n",
    "    # F1 Score\n",
    "    f1 = f1_score(all_labels, predictions)\n",
    "    # AUROC\n",
    "    auroc = roc_auc_score(all_labels, all_scores)\n",
    "    # AUPR\n",
    "    aupr = average_precision_score(all_labels, all_scores)\n",
    "    \n",
    "    # Detection Error\n",
    "    fpr, tpr, roc_thresholds = roc_curve(all_labels, all_scores)\n",
    "    detection_errors = 0.5 * (fpr + (1 - tpr))\n",
    "    detection_error = np.min(detection_errors)\n",
    "\n",
    "    return f1, auroc, aupr, detection_error, threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 20\n",
    "d_model = 8\n",
    "nhead = 4\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dim_feedforward = 32\n",
    "max_seq_length = 960\n",
    "dropout_rate = 0.1\n",
    "\n",
    "latents_d, _, recon_errors_d, inputs_d, outputs_d = evaluate_dae_vae(\n",
    "    model_class=TransformerAutoEncoder,\n",
    "    model_path=dae_model_save_path,\n",
    "    device='cuda',\n",
    "    test_dataloader=val_dataloader_traj,\n",
    "    input_dim=input_dim,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout_rate=dropout_rate,\n",
    "    criterion=masked_reconstruction_loss \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean reconstruction error:\", sum(recon_errors_d) / len(recon_errors_d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_d_ood, _, recon_errors_d_ood, _, _ = evaluate_dae_vae(\n",
    "    model_class=TransformerAutoEncoder,\n",
    "    model_path=dae_model_save_path,\n",
    "    device='cuda',\n",
    "    test_dataloader=val_ood_dataloader_traj\n",
    ")\n",
    "\n",
    "print(\"Mean reconstruction error:\", sum(recon_errors_d_ood) / len(recon_errors_d_ood))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_latent_representations_d = np.concatenate([latents_d, latents_d_ood], axis = 0)\n",
    "combined_recon_error_d = recon_errors_d + recon_errors_d_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_labels_d = [0] * len(latents_d) + [1] * len(latents_d_ood)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_d, ood_labels_d, uncertainty_type='OOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_d = np.percentile(combined_recon_error_d, 95)\n",
    "print(percentile_98_d)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_d, combined_recon_error_d, uncertainty_type='Reconstruction Error', threshold = percentile_98_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mean_features(inputs_d, outputs_d, num_features=19, num_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, auroc, aupr, detection_error, threshold = calculate_ood_metrics(recon_errors_d, recon_errors_d_ood, threshold_method='percentile', percentile=95)\n",
    "print(f\"Reconstruction Error F1 score (DAE): {f1:.4f}, AUROC: {auroc:.4f}, AUPR: {aupr}, Detection Error: {detection_error}, Threshold: {threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_v, latents_std, recon_errors_v, inputs_v, outputs_v = evaluate_dae_vae(\n",
    "    model_class=TransformerVariationalAutoEncoder,\n",
    "    model_path=vae_model_save_path,\n",
    "    device='cuda',\n",
    "    test_dataloader=val_dataloader_traj,\n",
    "    latent_dim=10,\n",
    "    input_dim=input_dim,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout_rate=dropout_rate,\n",
    "    criterion=masked_reconstruction_loss \n",
    ")\n",
    "\n",
    "print(\"Mean reconstruction error:\", sum(recon_errors_v) / len(recon_errors_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mean_features(inputs_v, outputs_v, num_features=19, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_v_ood, latents_std_ood, recon_errors_v_ood, _, _ = evaluate_dae_vae(\n",
    "    model_class=TransformerVariationalAutoEncoder,\n",
    "    model_path=vae_model_save_path,\n",
    "    device='cuda',\n",
    "    test_dataloader=val_ood_dataloader_traj,\n",
    "    latent_dim=10,\n",
    "    input_dim=input_dim,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout_rate=dropout_rate,\n",
    "    criterion=masked_reconstruction_loss\n",
    ")\n",
    "\n",
    "print(\"Mean reconstruction error:\", sum(recon_errors_v_ood) / len(recon_errors_v_ood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_latent_representations_v = np.concatenate([latents_v, latents_v_ood], axis = 0)\n",
    "combined_recon_error_v = recon_errors_v + recon_errors_v_ood\n",
    "combined_latents_std = np.concatenate([latents_std, latents_std_ood], axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_labels_v = [0] * len(latents_v) + [1] * len(latents_v_ood)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_v, ood_labels_v, uncertainty_type='ood label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_v = np.percentile(combined_recon_error_v, 95)\n",
    "print(percentile_98_v)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_v, combined_recon_error_v, uncertainty_type='recon_error', threshold = percentile_98_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(combined_latent_representations_v, combined_latents_std.mean(axis=1), uncertainty_type='latents_std', threshold = np.percentile(combined_latents_std, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_v, auroc_v, aupr_v, detection_error_v, threshold_v = calculate_ood_metrics(recon_errors_v, recon_errors_v_ood, threshold_method='percentile', percentile=95)\n",
    "print(f\"Reconstruction Error F1 score (VAE): {f1_v:.4f}, AUROC: {auroc_v:.4f}, AUPR: {aupr_v}, Detection Error: {detection_error_v}, Threshold: {threshold_v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_std, auroc_std, aupr_std, detection_error_std, threshold_std = calculate_ood_metrics(latents_std, latents_std_ood, threshold_method='percentile', percentile=75, reduce_method='mean')\n",
    "print(f\"STD F1 score (VAE): {f1_std:.4f}, AUROC: {auroc_std:.4f}, AUPR: {aupr_std}, Detection Error: {detection_error_std}, Threshold: {threshold_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_val_loss_e, recon_errors_e, inputs_e, outputs_e = evaluate_eae(\n",
    "    model_class=EvidentialTransformerDenoiseAutoEncoder, \n",
    "    model_path=eae_model_save_path,\n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_dataloader_traj, \n",
    "    lambda_reg=0.5, \n",
    "    offset=2.5, \n",
    "    device='cpu', \n",
    "    return_latent=False,\n",
    "    input_dim=20, \n",
    "    d_model=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "print(\"Mean reconstruction error:\", sum(recon_errors_e) / len(recon_errors_e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eae_model = EvidentialTransformerDenoiseAutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    d_model=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_e, val_aleatoric_uncertainties_e, val_epistemic_uncertainties_e, avg_aleatoric_uncertainty_e, avg_epistemic_uncertainty_e, latent_representations_eval_e, recon_error_e = evaluate_saved_model(\n",
    "    model_class=eae_model, \n",
    "    model_path=eae_model_save_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_dataloader_traj, \n",
    "    lambda_reg=0.5, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average:\", sum(recon_error_e) / len(recon_error_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ood_loss_e, val_ood_aleatoric_uncertainties_e, val_ood_epistemic_uncertainties_e, avg_ood_aleatoric_uncertainty_e, avg_ood_epistemic_uncertainty_e, latent_ood_representations_eval_e, recon_ood_error_le = evaluate_saved_model(\n",
    "    model_class=eae_model, \n",
    "    model_path=eae_model_save_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_ood_dataloader_traj, \n",
    "    lambda_reg=0.5, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_latent_representations_e = np.concatenate([latent_representations_eval_e, latent_ood_representations_eval_e], axis = 0)\n",
    "combined_recon_error_e = recon_error_e + recon_ood_error_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_val_epistemic_uncertainties_e = np.concatenate([val_epistemic_uncertainties_e, val_ood_epistemic_uncertainties_e], axis = 0)\n",
    "combined_val_aleatoric_uncertainties_e = np.concatenate([val_aleatoric_uncertainties_e, val_ood_aleatoric_uncertainties_e], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_labels_e = [0] * len(latent_representations_eval_e) + [1] * len(latent_ood_representations_eval_e)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_e, ood_labels_e, uncertainty_type='ood label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_e = np.percentile(combined_recon_error_e, 95)\n",
    "print(percentile_98_e)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_e, combined_recon_error_e, uncertainty_type='recon_error', threshold = percentile_98_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mean_features(inputs_e, outputs_e, num_features=19, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_e_uncertainty = np.percentile(combined_val_epistemic_uncertainties_e, 94.5)\n",
    "print(percentile_98_e_uncertainty)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_e, combined_val_epistemic_uncertainties_e, uncertainty_type='val_epistemic_uncertainties', threshold = percentile_98_e_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations_eval_e, val_epistemic_uncertainties_e, uncertainty_type='val_epistemic_uncertainties without ood', threshold = percentile_98_e_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_e_uncertainty_a = np.percentile(combined_val_aleatoric_uncertainties_e, 95)\n",
    "print(percentile_98_e_uncertainty_a)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_e, combined_val_aleatoric_uncertainties_e, uncertainty_type='val_aleatoric_uncertainties', threshold = percentile_98_e_uncertainty_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations_eval_e, val_aleatoric_uncertainties_e, uncertainty_type='val_aleatoric_uncertainties without ood', threshold = percentile_98_e_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_e, auroc_e, aupr_e, detection_error_e, threshold_e = calculate_ood_metrics(recon_errors_e, recon_ood_error_le, threshold_method='percentile', percentile=95)\n",
    "print(f\"Reconstruction Error F1 score (EAE): {f1_e:.4f}, AUROC: {auroc_e:.4f}, AUPR: {aupr_e}, Detection Error: {detection_error_e}, Threshold: {threshold_e:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_eu, auroc_eu, aupr_eu, detection_error_eu, threshold_eu = calculate_ood_metrics(val_epistemic_uncertainties_e, val_ood_epistemic_uncertainties_e, threshold_method='percentile', percentile=95)\n",
    "print(f\"Epistemic Uncertainty F1 score (EAE): {f1_eu:.4f}, AUROC: {auroc_eu:.4f}, AUPR: {aupr_eu}, Detection Error: {detection_error_eu}, Threshold: {threshold_eu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_au, auroc_au, aupr_au, detection_error_au, threshold_au = calculate_ood_metrics(val_aleatoric_uncertainties_e, val_ood_aleatoric_uncertainties_e, threshold_method='percentile', percentile=75)\n",
    "print(f\"Aleatoric Uncertainty F1 score (EAE): {f1_au:.4f}, AUROC: {auroc_au:.4f}, AUPR: {aupr_au}, Detection Error: {detection_error_au}, Threshold: {threshold_au:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key in val_dataset_traj.labels.keys():\n",
    "# for key in ['epoch', 'stopped', 'cog_c', 'aad', 'rot_c', 'speed_c', 'distance_c',\n",
    "#        'acc_c', 'cdd', 'dir_ccs', 'dist_ww', 'dist_ra',\n",
    "#        'dist_cl', 'dist_ma', 'traj_id', 'lon', 'lat', 'obj_id', 'datetime',\n",
    "#        'season', 'part_of_day', 'month_sin', 'month_cos', 'hour_sin',\n",
    "#        'hour_cos']:\n",
    "# for key in ['cog_c', 'aad', 'rot_c', 'speed_c', 'distance_c',\n",
    "#        'acc_c', 'cdd', 'dir_ccs', 'dist_ww', 'dist_ra',\n",
    "#        'dist_cl', 'dist_ma', 'lon', 'lat',\n",
    "#        'season', 'part_of_day', 'month_sin', 'month_cos', 'hour_sin',\n",
    "#        'hour_cos']:\n",
    "#     plot_tsne_with_uncertainty(latent_representations_eval_e, dataset_traj.labels[key], uncertainty_type=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
