{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "##> import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "#> import flower\n",
    "import flwr as fl\n",
    "\n",
    "#> import custom libraries\n",
    "from src.load import load_df_to_dataset\n",
    "from src.EAE import EvidentialTransformerDenoiseAutoEncoder, evidential_regression\n",
    "from src.client import train_and_evaluate_local, evaluate_saved_model, evaluate_local\n",
    "from src.datasets import TrajectoryDataset, generate_ood_data,clean_outliers_by_quantile\n",
    "from src.plot import plot_loss, plot_tsne_with_uncertainty, plot_uncertainty\n",
    "\n",
    "#> torch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#> Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "#plt.style.use(['science', 'grid', 'notebook'])  # , 'ieee'\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "#%matplotlib widget\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Define the dataset catalog\n",
    "assets_dir = root_dir.parents[3] / 'aistraj' / 'bin'/ 'tvt_assets'\n",
    "assets_dir = assets_dir.resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "    \n",
    "saved_model_dir = root_dir / 'models'\n",
    "saved_model_dir = saved_model_dir.resolve()\n",
    "print(f\"Assets Directory: {saved_model_dir}\")\n",
    "if not saved_model_dir.exists():\n",
    "    raise FileNotFoundError('Model directory not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(assets_dir, seq_len=960, batch_size=32):\n",
    "\n",
    "    # train dataset\n",
    "    train_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_train_df.parquet'\n",
    "    train_df_extend = load_df_to_dataset(train_pickle_path_extend).data\n",
    "\n",
    "    # validation dataset\n",
    "    validate_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_validate_df.parquet'\n",
    "    validate_df_extend = load_df_to_dataset(validate_pickle_path_extend).data\n",
    "    ood_df = generate_ood_data(validate_df_extend, ood_mean=10, ood_std=3)\n",
    "    #print(validate_df_extend)\n",
    "    # Define the list of features to discard\n",
    "    drop_features_list = ['epoch', 'datetime', 'obj_id', 'traj_id', 'stopped', 'curv', 'abs_ccs']\n",
    "    \n",
    "    # Specify columns to clean\n",
    "    columns_to_clean = ['speed_c', 'lon', 'lat']  # Specify columns to clean\n",
    "    cleaned_train_data = clean_outliers_by_quantile(train_df_extend, columns_to_clean, remove_na=False)\n",
    "    cleaned_val_data = clean_outliers_by_quantile(validate_df_extend, columns_to_clean, remove_na=False)\n",
    "   \n",
    "    # Create training and validation datasets\n",
    "    #df_extend = pd.concat([cleaned_train_data, cleaned_val_data])\n",
    "    df_extend = pd.concat([train_df_extend, validate_df_extend])\n",
    "    df_extend = df_extend.sort_index()\n",
    "    # Create training and validation datasets\n",
    "    train_dataset_traj = TrajectoryDataset(\n",
    "        cleaned_train_data,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        scaler_method='QuantileTransformer',\n",
    "        filter_less_seq_len = seq_len\n",
    "    )\n",
    "    val_dataset_traj = TrajectoryDataset(\n",
    "        cleaned_val_data,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        scaler_method='QuantileTransformer',\n",
    "        filter_less_seq_len = seq_len\n",
    "    )\n",
    "    val_ood_dataset_traj = TrajectoryDataset(\n",
    "        ood_df,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        filter_percent = None,\n",
    "        scaler = None,\n",
    "        filter_less_seq_len = None,\n",
    "        scaler_method = 'No_Scaler'\n",
    "    )    \n",
    "    #val_dataset_traj = train_dataset_traj\n",
    "    # dataset_traj = TrajectoryDataset(\n",
    "    #     df_extend,\n",
    "    #     seq_len=seq_len,\n",
    "    #     mode='ae',\n",
    "    #     drop_features_list=drop_features_list\n",
    "    # )\n",
    "    # train_dataset_traj = dataset_traj\n",
    "    # val_dataset_traj = dataset_traj\n",
    "    # Creating Data Loader\n",
    "    train_dataloader_traj = DataLoader(\n",
    "        train_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    val_dataloader_traj = DataLoader(\n",
    "        val_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    val_ood_dataloader_traj = DataLoader(\n",
    "        val_ood_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    return train_dataloader_traj, val_dataloader_traj, val_ood_dataloader_traj, train_dataset_traj.n_features, train_dataset_traj, val_dataset_traj, val_ood_dataset_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataloader_traj, val_dataloader_traj, val_ood_dataloader_traj, input_dim, train_dataset_traj, val_dataset_traj, val_ood_dataset_traj = load_datasets(assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_traj.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_traj.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_traj.labels[['traj_id', 'lon', 'lat', 'obj_id', 'datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_traj.dataframe[['traj_id', 'lon', 'lat', 'obj_id', 'datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['epoch', 'stopped', 'cog_c', 'aad', 'rot_c', 'speed_c', 'distance_c',\n",
    "       'acc_c', 'cdd', 'dir_ccs', 'dist_ww', 'dist_ra',\n",
    "       'dist_cl', 'dist_ma', 'traj_id', 'lon', 'lat', 'obj_id',\n",
    "       'season', 'part_of_day', 'month_sin', 'month_cos', 'hour_sin',\n",
    "       'hour_cos']:\n",
    "    print(key)\n",
    "    plt.figure()\n",
    "    train_dataset_traj.dataframe[key].hist(bins=30, grid=False, edgecolor='black')\n",
    "    #val_dataset_traj.labels[key].hist(bins=30, grid=False, edgecolor='black')\n",
    "    \n",
    "    max_index = train_dataset_traj.dataframe[key].idxmax()\n",
    "    max_value = train_dataset_traj.dataframe[key].max()\n",
    "    print(f\"max index and value: {max_index}\", max_value, (val_dataset_traj.dataframe[key] == max_value).sum())\n",
    "    # 获取最大值以及上下的值\n",
    "    # 获取索引列表\n",
    "    index_list = train_dataset_traj.dataframe[key].index.to_list()\n",
    "    \n",
    "    # 找到最大值索引在索引列表中的位置\n",
    "    max_pos = index_list.index(max_index)\n",
    "    \n",
    "    # 获取上下索引（防止越界）\n",
    "    start_pos = max(0, max_pos - 3)\n",
    "    end_pos = min(len(index_list) - 1, max_pos + 3)\n",
    "    \n",
    "    # 提取对应的行\n",
    "    result = train_dataset_traj.dataframe[[key, 'obj_id', 'traj_id', 'lon', 'lat','datetime']].loc[index_list[start_pos:end_pos + 1]]\n",
    "\n",
    "    print(result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticSampler(nn.Module):\n",
    "    \"\"\"We basically want to parametrize the sampling from the latent space\"\"\"\n",
    "\n",
    "    def __init__(self, deterministic=False):\n",
    "        super().__init__()\n",
    "        self.sampler = torch.distributions.Normal(loc=0, scale=1)\n",
    "        self.deterministic = deterministic\n",
    "\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        \"\"\"Return a normal sample value Z from the latent space given a mean and variance\"\"\"\n",
    "        # z_mean and z_log_var are mean and log-var estimates of the latent space\n",
    "        # under the assumption that the latent space is a gaussian normal\n",
    "        device = z_mean.device\n",
    "        # Scales and shifts the sampled values using the reparameterization trick\n",
    "        eps = self.sampler.sample(z_mean.shape).squeeze().to(device)\n",
    "        # print(eps.shape, z_log_var.shape, z_mean.shape)\n",
    "        return (\n",
    "            z_mean\n",
    "            if self.deterministic\n",
    "            else (z_mean + torch.exp(0.5 * z_log_var) * eps), torch.exp(0.5 * z_log_var)\n",
    "        )\n",
    "        \n",
    "class LSTMVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):\n",
    "        super(LSTMVAE, self).__init__()\n",
    "        self.latent_sampler = StochasticSampler(deterministic=False)\n",
    "        # Encoder\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)       # Mean of latent space\n",
    "        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)  # Log variance of latent space\n",
    "        #torch.nn.init.constant_(self.fc_log_var.weight, -0.5)\n",
    "        torch.nn.init.constant_(self.fc_log_var.weight, -1.0)\n",
    "        torch.nn.init.constant_(self.fc_log_var.bias, -1.0)\n",
    "        torch.nn.init.xavier_uniform_(self.fc_mu.weight)\n",
    "        self.bn_mu = nn.BatchNorm1d(latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder_fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_fc = nn.Linear(hidden_dim, input_dim)  # Map hidden_dim back to input_dim\n",
    "\n",
    "    def encode(self, x, lengths):\n",
    "        # Encode the input sequence\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, _) = self.encoder_lstm(packed_input)\n",
    "        h_n_last = h_n[-1]  # Use the last hidden state\n",
    "\n",
    "        # Compute latent space parameters\n",
    "        mu = self.fc_mu(h_n_last)\n",
    "        log_var = self.fc_log_var(h_n_last)\n",
    "        mu = self.bn_mu(mu)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        # Reparameterization trick: z = mu + epsilon * std\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + epsilon * std\n",
    "        return z, std\n",
    "\n",
    "    def decode(self, z, max_len, lengths):\n",
    "        # Map latent variable z to initial hidden state\n",
    "        hidden = self.decoder_fc(z).unsqueeze(0).repeat(self.decoder_lstm.num_layers, 1, 1)\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        # Create an all-zero input sequence for the decoder\n",
    "        decoder_input = torch.zeros(batch_size, max_len, hidden.size(2)).to(z.device)\n",
    "\n",
    "        # Decode sequence\n",
    "        packed_decoder_input = pack_padded_sequence(decoder_input, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.decoder_lstm(packed_decoder_input, (hidden, torch.zeros_like(hidden)))\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=max_len)\n",
    "\n",
    "        # Map hidden_dim back to input_dim\n",
    "        output = self.output_fc(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        max_len = x.size(1)\n",
    "        # Encoding\n",
    "        mu, log_var = self.encode(x, lengths)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        #z, _ = self.latent_sampler(mu, log_var)\n",
    "        # Decoding\n",
    "        output = self.decode(mu, max_len, lengths)\n",
    "\n",
    "        return output, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, seq_len):\n",
    "        \"\"\"\n",
    "        基于 CNN 的 VAE\n",
    "        参数:\n",
    "            input_dim: 时间序列的特征维度 (feature_dim)\n",
    "            latent_dim: 潜在空间的维度\n",
    "        \"\"\"\n",
    "        super(CNNVAE, self).__init__()\n",
    "        \n",
    "        # 编码器 (Encoder)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=5, stride=2, padding=2),  # (batch_size, 64, seq_len/2)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),        # (batch_size, 128, seq_len/4)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=5, stride=2, padding=2),       # (batch_size, 256, seq_len/8)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.seq_len = seq_len\n",
    "        self.compressed_seq_len = seq_len/8\n",
    "        \n",
    "        # 潜在空间的均值和对数方差\n",
    "        self.encoder_final = nn.Conv1d(256, latent_dim, kernel_size=1)       # (batch_size, latent_dim, compressed_seq_len)\n",
    "        self.encode_mean = nn.Linear(int(latent_dim*self.compressed_seq_len), latent_dim)\n",
    "        self.encode_logvar = nn.Linear(int(latent_dim*self.compressed_seq_len), latent_dim)\n",
    "        self.decode_adapter = nn.Linear(latent_dim, int(latent_dim*self.compressed_seq_len))\n",
    "\n",
    "        # 解码器 (Decoder)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(latent_dim, 128, kernel_size=5, stride=2, padding=2, output_padding=1),  # (batch_size, 128, seq_len/4)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),          # (batch_size, 64, seq_len/2)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(64, input_dim, kernel_size=5, stride=2, padding=2, output_padding=1),    # (batch_size, input_dim, seq_len)\n",
    "            #nn.Sigmoid()  # 输出值在 0 和 1 之间\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        重参数化技巧\n",
    "        参数:\n",
    "            mu: 潜在空间均值 (batch_size, latent_dim, compressed_seq_len)\n",
    "            log_var: 潜在空间对数方差 (batch_size, latent_dim, compressed_seq_len)\n",
    "        返回:\n",
    "            z: 采样的潜在变量 (batch_size, latent_dim, compressed_seq_len)\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var)  # 计算标准差\n",
    "        epsilon = torch.randn_like(std)  # 随机噪声\n",
    "        z = mu + epsilon * std  # 重参数化\n",
    "        return z, std\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        参数:\n",
    "            x: 输入时间序列 (batch_size, seq_len, feature_dim)\n",
    "        返回:\n",
    "            reconstructed: 重构的时间序列 (batch_size, seq_len, feature_dim)\n",
    "            mu: 潜在空间的均值 (batch_size, latent_dim, compressed_seq_len)\n",
    "            log_var: 潜在空间的对数方差 (batch_size, latent_dim, compressed_seq_len)\n",
    "        \"\"\"\n",
    "        # 转换输入形状以适配 Conv1d (batch_size, feature_dim, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # 编码\n",
    "        encoded = self.encoder(x)  # (batch_size, 256, compressed_seq_len)\n",
    "        encoded = self.encoder_final(encoded)  # (batch_size, latent_dim, compressed_seq_len)\n",
    "        encoded_output =  encoded.view(encoded.size(0), -1)\n",
    "        mu = self.encode_mean(encoded_output)\n",
    "        log_var = self.encode_logvar(encoded_output)\n",
    "        #print(mu.shape, log_var.shape)\n",
    "        # 重参数化\n",
    "        z, _ = self.reparameterize(mu, log_var)\n",
    "        z = self.decode_adapter(mu).view_as(encoded)\n",
    "        # 解码\n",
    "        reconstructed = self.decoder(z)  # (batch_size, input_dim, seq_len)\n",
    "        \n",
    "        # 恢复输出形状为 (batch_size, seq_len, feature_dim)\n",
    "        reconstructed = reconstructed.permute(0, 2, 1)\n",
    "        return reconstructed, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "        nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class TransformerAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, latent_dim, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, dropout_rate, output_dim=None):\n",
    "        super(TransformerAutoEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        if output_dim:\n",
    "            self.output_dim = output_dim\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = nn.Parameter(self._generate_positional_encoding(self.max_seq_length, d_model), requires_grad=False)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.encode_mean = nn.Linear(self.max_seq_length*d_model, latent_dim)\n",
    "        self.encode_logvar = nn.Linear(self.max_seq_length*d_model, latent_dim)\n",
    "        self.decode_adapter = nn.Linear(latent_dim, self.max_seq_length*d_model)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # Fully connected layers for encoding inputs and decoding outputs\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        self.output_fc = nn.Linear(d_model, output_dim or input_dim)   # Output 4x dimensions in order to separate mu, v, alpha, beta\n",
    "\n",
    "        self.bottleneck = nn.Linear(self.max_seq_length*d_model, 10)  # Optional, can skip if not needed\n",
    "        self.reconstruct = nn.Linear(10, self.max_seq_length*d_model)  # Project back to the original space\n",
    "        self.flatten = nn.Flatten(start_dim=1,end_dim=2)#\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(self.max_seq_length, d_model))\n",
    "        # Apply weight initialization\n",
    "        self.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, src, padding_mask=None, return_latent=False, noise_factor=0.05):\n",
    "        # add noise\n",
    "        noise = torch.randn_like(src) * noise_factor\n",
    "        noisy_src = src + noise\n",
    "        noisy_src = self.input_fc(noisy_src)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # add positional embedding\n",
    "        noisy_src += self.positional_encoding[:, :noisy_src.size(1), :]\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            # padding_mask:(batch_size, seq_len, input_dim)\n",
    "            padding_mask_timestep = padding_mask.any(dim=-1)  # [batch_size, seq_len]\n",
    "            mask_expanded = padding_mask_timestep.unsqueeze(-1).expand_as(noisy_src).bool()\n",
    "            # mask\n",
    "            noisy_src = torch.where(mask_expanded, noisy_src, torch.tensor(0.0, device=noisy_src.device))\n",
    "\n",
    "        # encode\n",
    "        encoded_memory = self.transformer_encoder(noisy_src)  \n",
    "        \n",
    "        encoded_output =  encoded_memory.view(encoded_memory.size(0), -1)\n",
    "        mu = self.encode_mean(encoded_output)\n",
    "        log_var = self.encode_logvar(encoded_output)\n",
    "        # Reparameterisation\n",
    "        z, _ = self.reparameterize(mu, log_var)\n",
    "        z = self.decode_adapter(mu).view_as(encoded_memory)\n",
    "        \n",
    "        # decode\n",
    "        #decoded_output = self.transformer_decoder(self.unflatten(hidden_to_decoder), encoded_memory)\n",
    "        decoded_output = self.transformer_decoder(z, z)\n",
    "        decoded_output = self.output_fc(decoded_output)  # Shape: (batch_size, seq_length, output_dim * 4)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            mask_expanded = padding_mask_timestep.unsqueeze(-1).expand(-1, -1, decoded_output.size(-1)).float()\n",
    "            #print(f\"decoded_output shape: {decoded_output.shape}\")\n",
    "            #print(f\"mask_expanded shape: {mask_expanded.shape}\")\n",
    "            decoded_output = decoded_output * mask_expanded\n",
    "\n",
    "        # Output mu, v, alpha, beta via Evidential Learning\n",
    "        # mu, logv, logalpha, logbeta = torch.chunk(decoded_output, 4, dim=2)\n",
    "        # v = F.softplus(logv) + 1e-6\n",
    "        # alpha = F.softplus(logalpha) + 1.0\n",
    "        # beta = F.softplus(logbeta) + 1e-6\n",
    "\n",
    "        # Return the encoded representation and decoded uncertainty outputs\n",
    "        # if return_latent:\n",
    "        #     return mu, v, alpha, beta, encoded_memory\n",
    "        # else:\n",
    "        #     return mu, v, alpha, beta\n",
    "        return decoded_output, mu, log_var\n",
    "\n",
    "\n",
    "    def _generate_positional_encoding(self, length, d_model):\n",
    "        position = torch.arange(length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pos_encoding = torch.zeros(length, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding.unsqueeze(0)  # add batch dimension\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "\n",
    "        std = torch.exp(0.5 * log_var)  # Calculate standard deviation\n",
    "        epsilon = torch.randn_like(std)  # Random noise\n",
    "        z = mu + epsilon * std  # Reparameterisation\n",
    "        return z, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, num_epochs=20):\n",
    "    model.train()\n",
    "    kl_weight = 0.1 \n",
    "    for epoch in range(num_epochs):\n",
    "        recon_losses = []\n",
    "        kl_losses = []\n",
    "        latent_reg = []\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_data = batch['inputs'].to(device)\n",
    "            masks = batch.get('input_masks', None).to(device)\n",
    "            if masks is not None:\n",
    "                lengths = masks.sum(dim=1).to('cpu')\n",
    "                masks = masks.to(device)\n",
    "                if masks.dim() == 2:\n",
    "                    masks_expend = masks.unsqueeze(-1).expand_as(batch_data)\n",
    "            else:\n",
    "                lengths = torch.tensor([960]*len(batch_data)).to('cpu')\n",
    "            optimizer.zero_grad()\n",
    "            #output, mu, log_var = model(batch_data, lengths)\n",
    "            #output, mu, log_var = model(batch_data)\n",
    "            output, mu, log_var = model(batch_data, masks_expend)\n",
    "            #print(output.shape, batch_data.shape )\n",
    "            recon_loss = criterion(output, batch_data,masks)\n",
    "            #recon_loss = ((batch_data - output) ** 2).mean(dim=1)\n",
    "            #kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / batch_data.size(0)\n",
    "            kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            #latent_regularization = torch.mean(torch.abs(mu)) + torch.mean(torch.abs(log_var))\n",
    "            #print(mu.mean(), log_var.mean())\n",
    "            loss = recon_loss + kl_loss#*kl_weight #+ latent_regularization\n",
    "            recon_losses.append(recon_loss.item())\n",
    "            kl_losses.append(kl_loss.item())\n",
    "            #latent_reg.append(latent_regularization.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        kl_weight = min(1.0, kl_weight + 0.1)\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Recon Loss: {np.mean(recon_losses):.4f}, KL Loss: {np.mean(kl_losses):.4f}\")#, LR Loss: {np.mean(latent_reg):.4f}\")\n",
    "\n",
    "def reconstruction_error_per_sample(x, x_reconstructed, reduction='mean'):\n",
    "    # Ensure input and reconstructed tensors have the same shape\n",
    "    assert x.shape == x_reconstructed.shape, \"Input and reconstructed tensors must have the same shape\"\n",
    "\n",
    "    # Compute element-wise squared error\n",
    "    errors = (x - x_reconstructed) ** 2  # (batch_size, seq_len, channels)\n",
    "\n",
    "    # Aggregate errors across the time and channel dimensions\n",
    "    if reduction == 'mean':\n",
    "        errors_per_sample = errors.mean(dim=(1, 2))  # Compute the mean error per sample\n",
    "    elif reduction == 'sum':\n",
    "        errors_per_sample = errors.sum(dim=(1, 2))  # Compute the total error per sample\n",
    "    else:\n",
    "        raise ValueError(\"Reduction must be 'mean' or 'sum'\")\n",
    "\n",
    "    return errors_per_sample\n",
    "    \n",
    "def get_latent_space(model, dataloader, device):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    latents_std = []\n",
    "    recon_error = []\n",
    "    #labels = []  # If there are labels, they can be used for visual distinction\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_data = batch['inputs'].to(device)\n",
    "            masks = batch.get('input_masks', None)\n",
    "            if masks is not None:\n",
    "                lengths = masks.sum(dim=1).to('cpu')\n",
    "            else:\n",
    "                lengths = torch.tensor([960]*len(batch_data)).to('cpu')\n",
    "            batch_data = batch_data.to(device)\n",
    "            #recon_data, mu, log_var = model(batch_data, lengths)\n",
    "            recon_data, mu, log_var = model(batch_data)\n",
    "            latent, std = model.reparameterize(mu, log_var)\n",
    "            #print(log_var)\n",
    "            #print(latent.shape, std.shape)\n",
    "            latents.append(mu.cpu().numpy())\n",
    "            latents_std.append(std.cpu().numpy())\n",
    "            loss = reconstruction_error_per_sample(batch_data, recon_data)\n",
    "            recon_error.extend(loss.cpu().numpy())\n",
    "            # If you have tags, you can add them here\n",
    "            # labels.append(batch_labels)\n",
    "\n",
    "    latents = np.vstack(latents)\n",
    "    latents_std = np.vstack(latents_std)\n",
    "    #recon_error = np.vstack(recon_error)\n",
    "    print(f\"Get Latent Space Done\")\n",
    "    return latents, latents_std, recon_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "    \n",
    "def load_model(model, path, device='cpu'):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded from {path}\") \n",
    "    \n",
    "def masked_reconstruction_loss(original, reconstructed, mask, offset=2.5, batch_mean=True):\n",
    "\n",
    "    # Compute element-wise Mean Squared Error (MSE)\n",
    "    error = F.mse_loss(reconstructed, original, reduction=\"none\")  # (batch_size, seq_len, feature_dim)\n",
    "    error = error.mean(dim=-1)  # Average across feature dimension (batch_size, seq_len)\n",
    "\n",
    "    # Apply the mask to keep only valid time steps\n",
    "    masked_error = error * mask  # (batch_size, seq_len)\n",
    "\n",
    "    # Compute the mean loss over valid time steps for each sample\n",
    "    loss = masked_error.sum(dim=1) / mask.sum(dim=1)  # Weighted average per sample\n",
    "\n",
    "    # Compute batch mean if specified\n",
    "    if batch_mean:\n",
    "        loss = loss.mean()  # Average over batch\n",
    "\n",
    "    # Add offset to the final loss\n",
    "    loss = loss + offset\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = 20\n",
    "hidden_dim = 50\n",
    "latent_dim = 10\n",
    "num_layers = 2\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "# Create model\n",
    "#model = LSTMVAE(input_dim, hidden_dim, latent_dim, num_layers).to(device)\n",
    "#model = CNNVAE(input_dim, latent_dim, seq_len=960).to(device)\n",
    "model = TransformerAutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    d_model=8,\n",
    "    latent_dim = latent_dim,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    ").to(device)\n",
    "\n",
    "criterion = masked_reconstruction_loss #nn.MSELoss() #masked_reconstruction_loss #nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "save_model_path =  saved_model_dir + 'vae_model_qt_960_20.pth'\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataloader_traj, criterion, optimizer, num_epochs=num_epochs, device=device)\n",
    "save_model(model, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_traj = val_dataloader_traj\n",
    "dataset_traj = val_dataset_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space, latents_std, recon_error = get_latent_space(model, dataloader_traj,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representations = latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recon_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(recon_error, bins=10, edgecolor='black', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_99 = np.percentile(recon_error, 99.5)\n",
    "print(percentile_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(recon_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations, recon_error, uncertainty_type='recon_error', threshold = percentile_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations, latents_std.mean(axis=1), uncertainty_type='latents_std', threshold = np.percentile(latents_std, 99.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=1, random_state=42)\n",
    "label_pos = tsne.fit_transform(dataset_traj.labels[['lon', 'lat']])\n",
    "tsne = TSNE(n_components=1, random_state=42)\n",
    "label_obj = tsne.fit_transform(dataset_traj.labels[['traj_id', 'obj_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations, label_pos, uncertainty_type='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations, label_obj, uncertainty_type='obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representations_ood, latents_std_ood, recon_error_ood = get_latent_space(model, val_ood_dataloader_traj,criterion)\n",
    "combined_latent_representations = np.concatenate([latent_representations, latent_representations_ood], axis = 0)\n",
    "combined_recon_error = recon_error + recon_error_ood\n",
    "combined_latents_std = np.concatenate([latents_std, latents_std_ood], axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98 = np.percentile(combined_recon_error, 98)\n",
    "print(percentile_98)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations, combined_recon_error, uncertainty_type='recon_error', threshold = percentile_98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(combined_latent_representations, combined_latents_std.mean(axis=1), uncertainty_type='latents_std', threshold = np.percentile(combined_latents_std.mean(axis=1), 99.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_labels = [0] * len(latent_representations) + [1] * len(latent_representations_ood)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations, ood_labels, uncertainty_type='ood label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(model, save_model_path, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key in val_dataset_traj.labels.keys():\n",
    "for key in ['epoch', 'stopped', 'cog_c', 'aad', 'rot_c', 'speed_c', 'distance_c',\n",
    "       'acc_c', 'cdd', 'dir_ccs', 'dist_ww', 'dist_ra',\n",
    "       'dist_cl', 'dist_ma', 'traj_id', 'lon', 'lat', 'obj_id', 'datetime',\n",
    "       'season', 'part_of_day', 'month_sin', 'month_cos', 'hour_sin',\n",
    "       'hour_cos']:\n",
    "    plot_tsne_with_uncertainty(latent_representations, dataset_traj.labels[key], uncertainty_type=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.sqrt(val_dataset_traj.labels[\"lon\"]**2 + val_dataset_traj.labels[\"lat\"]**2)\n",
    "plot_tsne_with_uncertainty(latent_representations, dist, uncertainty_type=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
