{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "##> import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "#> import flower\n",
    "import flwr as fl\n",
    "\n",
    "#> import custom libraries\n",
    "from src.load import load_df_to_dataset\n",
    "from src.EAE import EvidentialTransformerDenoiseAutoEncoder, evidential_regression\n",
    "from src.client import train_and_evaluate_local, evaluate_saved_model, evaluate_local\n",
    "from src.datasets import TrajectoryDataset, generate_ood_data,clean_outliers_by_quantile\n",
    "from src.plot import plot_loss, plot_tsne_with_uncertainty, plot_uncertainty\n",
    "\n",
    "#> torch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#> Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "# import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "#plt.style.use(['science', 'grid', 'notebook'])  # , 'ieee'\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "# %matplotlib inline\n",
    "#%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Define the dataset catalog\n",
    "assets_dir = root_dir.parents[3] / 'aistraj' / 'bin'/ 'tvt_assets'\n",
    "assets_dir = assets_dir.resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "    \n",
    "saved_model_dir = root_dir / 'models'\n",
    "saved_model_dir = saved_model_dir.resolve()\n",
    "print(f\"Assets Directory: {saved_model_dir}\")\n",
    "if not saved_model_dir.exists():\n",
    "    raise FileNotFoundError('Model directory not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(assets_dir, seq_len=960, batch_size=32):\n",
    "\n",
    "    # train dataset\n",
    "    train_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_train_df.parquet'\n",
    "    train_df_extend = load_df_to_dataset(train_pickle_path_extend).data\n",
    "\n",
    "    # validation dataset\n",
    "    validate_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_validate_df.parquet'\n",
    "    validate_df_extend = load_df_to_dataset(validate_pickle_path_extend).data\n",
    "    ood_df = generate_ood_data(validate_df_extend, ood_mean=10, ood_std=3)\n",
    "    #print(validate_df_extend)\n",
    "    # Define the list of features to discard\n",
    "    drop_features_list = ['epoch', 'datetime', 'obj_id', 'traj_id', 'stopped', 'curv', 'abs_ccs']\n",
    "    \n",
    "    # Specify columns to clean\n",
    "    columns_to_clean = ['speed_c', 'lon', 'lat']  # Specify columns to clean\n",
    "    cleaned_train_data = clean_outliers_by_quantile(train_df_extend, columns_to_clean, remove_na=False)\n",
    "    cleaned_val_data = clean_outliers_by_quantile(validate_df_extend, columns_to_clean, remove_na=False)\n",
    "   \n",
    "    # Create training and validation datasets\n",
    "    #df_extend = pd.concat([cleaned_train_data, cleaned_val_data])\n",
    "    df_extend = pd.concat([train_df_extend, validate_df_extend])\n",
    "    df_extend = df_extend.sort_index()\n",
    "    # Create training and validation datasets\n",
    "    train_dataset_traj = TrajectoryDataset(\n",
    "        cleaned_train_data,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        scaler_method='QuantileTransformer',\n",
    "        filter_less_seq_len = seq_len\n",
    "    )\n",
    "    val_dataset_traj = TrajectoryDataset(\n",
    "        cleaned_val_data,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        scaler_method='QuantileTransformer',\n",
    "        filter_less_seq_len = seq_len\n",
    "    )\n",
    "    val_ood_dataset_traj = TrajectoryDataset(\n",
    "        ood_df,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        filter_percent = None,\n",
    "        scaler = None,\n",
    "        filter_less_seq_len = None,\n",
    "        scaler_method = 'No_Scaler'\n",
    "    )    \n",
    "\n",
    "    train_dataloader_traj = DataLoader(\n",
    "        train_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    val_dataloader_traj = DataLoader(\n",
    "        val_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    val_ood_dataloader_traj = DataLoader(\n",
    "        val_ood_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    return train_dataloader_traj, val_dataloader_traj, val_ood_dataloader_traj, train_dataset_traj.n_features, train_dataset_traj, val_dataset_traj, val_ood_dataset_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataloader_traj, val_dataloader_traj, val_ood_dataloader_traj, input_dim, train_dataset_traj, val_dataset_traj, val_ood_dataset_traj = load_datasets(assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_traj.input_masks.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_traj.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ood_dataset_traj.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_traj.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_traj.labels[['traj_id', 'lon', 'lat', 'obj_id', 'datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ood_dataset_traj.dataframe[['traj_id', 'lon', 'lat', 'obj_id', 'datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_traj.dataframe[['traj_id', 'lon', 'lat', 'obj_id', 'datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['epoch', 'stopped', 'cog_c', 'aad', 'rot_c', 'speed_c', 'distance_c',\n",
    "       'acc_c', 'cdd', 'dir_ccs', 'dist_ww', 'dist_ra',\n",
    "       'dist_cl', 'dist_ma', 'traj_id', 'lon', 'lat', 'obj_id',\n",
    "       'season', 'part_of_day', 'month_sin', 'month_cos', 'hour_sin',\n",
    "       'hour_cos']:\n",
    "    print(key)\n",
    "    plt.figure()\n",
    "    train_dataset_traj.dataframe[key].hist(bins=30, grid=False, edgecolor='black')\n",
    "    #val_dataset_traj.labels[key].hist(bins=30, grid=False, edgecolor='black')\n",
    "    \n",
    "    max_index = train_dataset_traj.dataframe[key].idxmax()\n",
    "    max_value = train_dataset_traj.dataframe[key].max()\n",
    "    print(f\"max index and value: {max_index}\", max_value, (val_dataset_traj.dataframe[key] == max_value).sum())\n",
    "    # Get the maximum value and the values above and below\n",
    "    # Get the index list\n",
    "    index_list = train_dataset_traj.dataframe[key].index.to_list()\n",
    "    \n",
    "    # Find the position of the maximum index in the index list\n",
    "    max_pos = index_list.index(max_index)\n",
    "    \n",
    "    # Get the upper and lower indexes (to prevent out-of-bounds)\n",
    "    start_pos = max(0, max_pos - 3)\n",
    "    end_pos = min(len(index_list) - 1, max_pos + 3)\n",
    "    \n",
    "    # Extract the corresponding row\n",
    "    result = train_dataset_traj.dataframe[[key, 'obj_id', 'traj_id', 'lon', 'lat','datetime']].loc[index_list[start_pos:end_pos + 1]]\n",
    "\n",
    "    print(result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "        nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class TransformerAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, dropout_rate, output_dim=None):\n",
    "        super(TransformerAutoEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        if output_dim:\n",
    "            self.output_dim = output_dim\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = nn.Parameter(self._generate_positional_encoding(self.max_seq_length, d_model), requires_grad=False)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # Fully connected layers for encoding inputs and decoding outputs\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        self.output_fc = nn.Linear(d_model, output_dim or input_dim)   # Output 4x dimensions in order to separate mu, v, alpha, beta\n",
    "\n",
    "        self.bottleneck = nn.Linear(self.max_seq_length*d_model, 10)  # Optional, can skip if not needed\n",
    "        self.reconstruct = nn.Linear(10, self.max_seq_length*d_model)  # Project back to the original space\n",
    "        self.flatten = nn.Flatten(start_dim=1,end_dim=2)#\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(self.max_seq_length, d_model))\n",
    "        # Apply weight initialization\n",
    "        self.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, src, padding_mask=None, return_latent=False, noise_factor=0.05):\n",
    "        # add noise\n",
    "        noise = torch.randn_like(src) * noise_factor\n",
    "        noisy_src = src + noise\n",
    "        noisy_src = self.input_fc(noisy_src)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # add positional embedding\n",
    "        noisy_src += self.positional_encoding[:, :noisy_src.size(1), :]\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            # padding_mask:(batch_size, seq_len, input_dim)\n",
    "            padding_mask_timestep = padding_mask.any(dim=-1)  # [batch_size, seq_len]\n",
    "            mask_expanded = padding_mask_timestep.unsqueeze(-1).expand_as(noisy_src).bool()\n",
    "            # mask\n",
    "            noisy_src = torch.where(mask_expanded, noisy_src, torch.tensor(0.0, device=noisy_src.device))\n",
    "\n",
    "        # encode\n",
    "        encoded_memory = self.transformer_encoder(noisy_src)  \n",
    "        \n",
    "        # Bottleneck layer (optional)\n",
    "        #hidden_representation = self.bottleneck(self.flatten(encoded_memory))  # Shape: (batch_size, seq_length, d_model)\n",
    "        # Decoder: Use hidden representation to reconstruct\n",
    "        #hidden_to_decoder = self.reconstruct(hidden_representation)  # Project back to (batch_size, seq_length, d_model)\n",
    "        \n",
    "        # decode\n",
    "        #decoded_output = self.transformer_decoder(self.unflatten(hidden_to_decoder), encoded_memory)\n",
    "        decoded_output = self.transformer_decoder(encoded_memory, encoded_memory)\n",
    "        decoded_output = self.output_fc(decoded_output)  # Shape: (batch_size, seq_length, output_dim * 4)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            mask_expanded = padding_mask_timestep.unsqueeze(-1).expand(-1, -1, decoded_output.size(-1)).float()\n",
    "            #print(f\"decoded_output shape: {decoded_output.shape}\")\n",
    "            #print(f\"mask_expanded shape: {mask_expanded.shape}\")\n",
    "            decoded_output = decoded_output * mask_expanded\n",
    "\n",
    "        # Output mu, v, alpha, beta via Evidential Learning\n",
    "        # mu, logv, logalpha, logbeta = torch.chunk(decoded_output, 4, dim=2)\n",
    "        # v = F.softplus(logv) + 1e-6\n",
    "        # alpha = F.softplus(logalpha) + 1.0\n",
    "        # beta = F.softplus(logbeta) + 1e-6\n",
    "\n",
    "        # Return the encoded representation and decoded uncertainty outputs\n",
    "        # if return_latent:\n",
    "        #     return mu, v, alpha, beta, encoded_memory\n",
    "        # else:\n",
    "        #     return mu, v, alpha, beta\n",
    "        return decoded_output, encoded_memory\n",
    "\n",
    "\n",
    "    def _generate_positional_encoding(self, length, d_model):\n",
    "        position = torch.arange(length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pos_encoding = torch.zeros(length, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding.unsqueeze(0)  # add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_data = batch['inputs'].to(device)\n",
    "            masks = batch.get('input_masks', None).to(device)\n",
    "            if masks is not None:\n",
    "                lengths = masks.sum(dim=1).to('cpu')\n",
    "                masks = masks.to(device)\n",
    "                if masks.dim() == 2:\n",
    "                    masks_expend = masks.unsqueeze(-1).expand_as(batch_data)\n",
    "            else:\n",
    "                lengths = torch.tensor([960]*len(batch_data)).to('cpu')\n",
    "            optimizer.zero_grad()\n",
    "            #output, _ = model(batch_data, lengths)\n",
    "            #output, _ = model(batch_data)\n",
    "            output, _ = model(batch_data, masks_expend)\n",
    "            #print(output.shape, batch_data.shape)\n",
    "            loss = criterion(output, batch_data, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "    \n",
    "def load_model(model, path, device='cpu'):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded from {path}\")    \n",
    "\n",
    "def masked_reconstruction_loss(original, reconstructed, mask, offset=2.5, batch_mean=True):\n",
    "    \"\"\"\n",
    "    original: 原始时间序列 (batch_size, seq_len, feature_dim)\n",
    "    reconstructed: 重建时间序列 (batch_size, seq_len, feature_dim)\n",
    "    mask: 时间序列的有效时间步 mask (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    # Time-step-by-time-step calculation error (MSE)\n",
    "    error = F.mse_loss(reconstructed, original, reduction=\"none\")  # (batch_size, seq_len, feature_dim)\n",
    "    error = error.mean(dim=-1)  # Averaging by feature dimension to get error per time step (batch_size, seq_len)\n",
    "\n",
    "    # Apply mask\n",
    "    masked_error = error * mask  # Keep only the error of valid time steps\n",
    "\n",
    "    # Average over valid time steps\n",
    "    loss = masked_error.sum(dim=1) / mask.sum(dim=1)  # Calculate weighted average per sample\n",
    "    if batch_mean == True:\n",
    "        loss = loss.mean()  # Average the samples in the batch\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    loss = loss + offset\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def reconstruction_error_per_sample(x, x_reconstructed, reduction='mean'):\n",
    "    # Check for shape consistency\n",
    "    assert x.shape == x_reconstructed.shape, \"Input and reconstructed tensors must have the same shape\"\n",
    "    \n",
    "    # Calculate error element by element\n",
    "    errors = (x - x_reconstructed) ** 2  # (batch_size, seq_len, channels)\n",
    "    \n",
    "    # Summarise errors in seq_len and channels dimensions\n",
    "    if reduction == 'mean':\n",
    "        errors_per_sample = errors.mean(dim=(1, 2))  # Averaging by time and channel\n",
    "    elif reduction == 'sum':\n",
    "        errors_per_sample = errors.sum(dim=(1, 2))  # Summing over time and channels\n",
    "    else:\n",
    "        raise ValueError(\"Reduction must be 'mean' or 'sum'\")\n",
    "    \n",
    "    return errors_per_sample\n",
    "    \n",
    "def get_latent_space(model, dataloader, device):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    recon_error = []\n",
    "    #labels = []  # If there are labels, they can be used for visual distinction\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_data = batch['inputs'].to(device)\n",
    "            masks = batch.get('input_masks', None)\n",
    "            if masks is not None:\n",
    "                lengths = masks.sum(dim=1).to('cpu')\n",
    "                masks = masks.to(device)\n",
    "                if masks.dim() == 2:\n",
    "                    masks_expend = masks.unsqueeze(-1).expand_as(batch_data)\n",
    "            else:\n",
    "                lengths = torch.tensor([960]*len(batch_data)).to('cpu')\n",
    "            batch_data = batch_data.to(device)\n",
    "            #recon_data, latent = model(batch_data, lengths)\n",
    "            #recon_data, latent = model(batch_data)\n",
    "            recon_data, latent = model(batch_data,masks)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "            loss = masked_reconstruction_loss(batch_data.to(device), recon_data.to(device), masks.to(device), batch_mean=False)\n",
    "            #print(loss.shape)\n",
    "            #recon_error+=loss.cpu().numpy())\n",
    "            #recon_error.extend(loss.cpu().numpy())\n",
    "            recon_error.extend(loss.cpu().numpy())\n",
    "            # If you have tags, you can add them here\n",
    "            # labels.append(batch_labels)\n",
    "\n",
    "    latents = np.vstack(latents)\n",
    "    #recon_error = np.vstack(recon_error)\n",
    "    print(f\"Get Latent Space Done\")\n",
    "    return latents, recon_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = 20\n",
    "hidden_dim = 50\n",
    "latent_dim = 8\n",
    "num_layers = 2\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "# Create model\n",
    "model = TransformerAutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    d_model=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    ").to(device)\n",
    "criterion = masked_reconstruction_loss# nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "save_model_path = saved_model_dir + 'dae_model_qt_960_50e.pth'\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataloader_traj, criterion, optimizer, num_epochs=num_epochs, device=device)\n",
    "save_model(model, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representations, recon_error = get_latent_space(model, val_dataloader_traj,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recon_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_error[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=1, random_state=42)\n",
    "label_pos = tsne.fit_transform(val_dataset_traj.labels[['lon', 'lat']])\n",
    "tsne = TSNE(n_components=1, random_state=42)\n",
    "label_obj = tsne.fit_transform(val_dataset_traj.labels[['traj_id', 'obj_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(recon_error, bins=10, edgecolor='black', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_99 = np.percentile(recon_error, 99)\n",
    "print(percentile_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(recon_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations, recon_error, uncertainty_type='recon_error', threshold = percentile_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations, label_pos, uncertainty_type='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations, label_obj, uncertainty_type='obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representations_ood, recon_error_ood = get_latent_space(model, val_ood_dataloader_traj,criterion)\n",
    "combined_latent_representations = np.concatenate([latent_representations, latent_representations_ood], axis = 0)\n",
    "combined_recon_error = recon_error + recon_error_ood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_99 = np.percentile(combined_recon_error, 99)\n",
    "print(percentile_99)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations, combined_recon_error, uncertainty_type='recon_error', threshold = percentile_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_labels = [0] * len(latent_representations) + [1] * len(latent_representations_ood)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations, ood_labels, uncertainty_type='ood label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(model, save_model_path, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key in val_dataset_traj.labels.keys():\n",
    "for key in ['epoch', 'stopped', 'cog_c', 'aad', 'rot_c', 'speed_c', 'distance_c',\n",
    "       'acc_c', 'cdd', 'dir_ccs', 'dist_ww', 'dist_ra',\n",
    "       'dist_cl', 'dist_ma', 'traj_id', 'lon', 'lat', 'obj_id', 'datetime',\n",
    "       'season', 'part_of_day', 'month_sin', 'month_cos', 'hour_sin',\n",
    "       'hour_cos']:\n",
    "    plot_tsne_with_uncertainty(latent_representations, val_dataset_traj.labels[key], uncertainty_type=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.sqrt(val_dataset_traj.labels[\"lon\"]**2 + val_dataset_traj.labels[\"lat\"]**2)\n",
    "plot_tsne_with_uncertainty(latent_representations, dist, uncertainty_type=key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
