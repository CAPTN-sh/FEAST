{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "##> import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "from typing import OrderedDict\n",
    "\n",
    "\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "#> import flower\n",
    "import flwr as fl\n",
    "from flwr.common import Context\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner, DirichletPartitioner\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "#> import custom libraries\n",
    "from src.load import load_df_to_dataset\n",
    "from src.EAE import EvidentialTransformerDenoiseAutoEncoder, evidential_regression\n",
    "from src.client import train_and_evaluate_local, evaluate_saved_model\n",
    "from src.datasets import TrajectoryDataset, clean_outliers_by_quantile, generate_ood_data\n",
    "from src.plot import plot_tsne_with_uncertainty, visualize_mean_features\n",
    "\n",
    "#> torch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, roc_curve\n",
    "\n",
    "#> Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "# plt.style.use(['science', 'grid', 'notebook', 'ieee'])  # , 'ieee'\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# %matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the dataset catalog\n",
    "assets_dir = root_dir.parents[3] / 'aistraj' / 'bin'/ 'tvt_assets'\n",
    "assets_dir = assets_dir.resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "    \n",
    "saved_model_dir = root_dir / 'models'\n",
    "saved_model_dir = saved_model_dir.resolve()\n",
    "print(f\"Assets Directory: {saved_model_dir}\")\n",
    "if not saved_model_dir.exists():\n",
    "    raise FileNotFoundError('Model directory not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # setup_environment()\n",
    "if multiprocessing.get_start_method(allow_none=True) != \"spawn\":\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "\n",
    "# Define the dataset catalog\n",
    "assets_dir = Path(\"/data1/aistraj/bin/tvt_assets\").resolve()\n",
    "print(f\"Assets Directory: {assets_dir}\")\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "\n",
    "# Set the working directory to the 'src' directory, which contains only the code.\n",
    "code_dir = root_dir / 'src'\n",
    "code_dir = code_dir.resolve()\n",
    "print(f\"Code Directory: {code_dir}\")\n",
    "if not code_dir.exists():\n",
    "    raise FileNotFoundError('Code directory not found')\n",
    "\n",
    "excludes = [\"data\", \"*.pyc\", \"__pycache__\"\n",
    "]\n",
    "\n",
    "ray_init_args = {\n",
    "    \"runtime_env\": {\n",
    "        #\"working_dir\": str(code_dir),\n",
    "        \"py_modules\": [str(code_dir)],\n",
    "        \"excludes\": [str(code_dir / file) for file in excludes]\n",
    "    },\n",
    "    \"include_dashboard\": False,\n",
    "    #\"num_cpus\": 4,\n",
    "    # \"local_mode\": True\n",
    "}\n",
    "\n",
    "num_clients = 4\n",
    "\n",
    "# config = {\n",
    "#     \"lambda_reg\": 1,     \n",
    "#     \"num_epochs\": 1,        \n",
    "#     \"offset\": 2.5,       \n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, roc_curve\n",
    "\n",
    "def calculate_ood_metrics(id_scores, ood_scores, threshold_method='percentile', percentile=95, k=1.0, reduce_method='mean'):\n",
    "    # 1. Ensure input data is a NumPy array\n",
    "    id_scores = np.array(id_scores)\n",
    "    ood_scores = np.array(ood_scores)\n",
    "    \n",
    "    # 2. Dimensionality reduction if the input is 2D (batch_size, latent_dim)\n",
    "    if id_scores.ndim > 1:\n",
    "        if reduce_method == 'mean':\n",
    "            id_scores = np.mean(id_scores, axis=1)\n",
    "            ood_scores = np.mean(ood_scores, axis=1)\n",
    "        elif reduce_method == 'max':\n",
    "            id_scores = np.max(id_scores, axis=1)\n",
    "            ood_scores = np.max(ood_scores, axis=1)\n",
    "        elif reduce_method == 'l2':\n",
    "            id_scores = np.linalg.norm(id_scores, axis=1)\n",
    "            ood_scores = np.linalg.norm(ood_scores, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid reduce_method. Available options are 'mean', 'max', 'l2'.\")\n",
    "\n",
    "    # 3. Calculate the threshold\n",
    "    if threshold_method == 'percentile':\n",
    "        threshold = np.percentile(id_scores, percentile)\n",
    "    elif threshold_method == 'mean_std':\n",
    "        threshold = np.mean(id_scores) + k * np.std(id_scores)\n",
    "    else:\n",
    "        raise ValueError(\"threshold_method must be 'percentile' or 'mean_std'\")\n",
    "    \n",
    "    # 4. Concatenate ID and OOD scores\n",
    "    all_scores = np.concatenate([id_scores, ood_scores], axis=0)\n",
    "    \n",
    "    # 5. Create labels (ID is 0, OOD is 1)\n",
    "    labels_id = np.zeros(len(id_scores))  # ID labels\n",
    "    labels_ood = np.ones(len(ood_scores)) # OOD labels\n",
    "    all_labels = np.concatenate([labels_id, labels_ood], axis=0)\n",
    "    \n",
    "    # 6. Generate predictions based on threshold\n",
    "    predictions = (all_scores > threshold).astype(int)\n",
    "    \n",
    "    # 7. Calculate metrics\n",
    "    # F1 Score\n",
    "    f1 = f1_score(all_labels, predictions)\n",
    "    # AUROC\n",
    "    auroc = roc_auc_score(all_labels, all_scores)\n",
    "    # AUPR\n",
    "    aupr = average_precision_score(all_labels, all_scores)\n",
    "    \n",
    "    # Detection Error\n",
    "    fpr, tpr, roc_thresholds = roc_curve(all_labels, all_scores)\n",
    "    detection_errors = 0.5 * (fpr + (1 - tpr))\n",
    "    detection_error = np.min(detection_errors)\n",
    "\n",
    "    return f1, auroc, aupr, detection_error, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model_save_path = saved_model_dir + 'eae_model_sequential_lambda05_960.pth'\n",
    "global_model_save_path = saved_model_dir + 'feae_model_global_lambda05_random_960.pth'\n",
    "local_model_save_path = saved_model_dir + 'eae_model_qt_lambda05_960.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_eval(assets_dir, seq_len=960, batch_size=32):\n",
    "    \n",
    "     # train dataset\n",
    "    train_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_train_df.parquet'\n",
    "    train_df_extend = load_df_to_dataset(train_pickle_path_extend).data\n",
    "\n",
    "    # validation dataset\n",
    "    validate_pickle_path_extend = assets_dir / 'extended' / 'cleaned_extended_validate_df.parquet'\n",
    "    validate_df_extend = load_df_to_dataset(validate_pickle_path_extend).data\n",
    "    ood_df = generate_ood_data(validate_df_extend, ood_mean=10, ood_std=3)\n",
    "    #print (ood_df.shape)\n",
    "\n",
    "    # Define the list of features to discard\n",
    "    drop_features_list = ['epoch', 'datetime', 'obj_id', 'traj_id', 'stopped', 'curv', 'abs_ccs']\n",
    "    columns_to_clean = ['speed_c', 'lon', 'lat']  # Specify columns to clean\n",
    "    \n",
    "    cleaned_train_data = clean_outliers_by_quantile(train_df_extend, columns_to_clean, remove_na=False)\n",
    "    cleaned_val_data = clean_outliers_by_quantile(validate_df_extend, columns_to_clean, remove_na=False)\n",
    "    \n",
    "    df_extend = pd.concat([cleaned_train_data, cleaned_val_data])\n",
    "    df_extend = df_extend.sort_index()\n",
    "    \n",
    "    val_dataset_traj = TrajectoryDataset(\n",
    "        cleaned_val_data,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        scaler_method='QuantileTransformer',\n",
    "        filter_less_seq_len=6\n",
    "        #categorical_features=['season']\n",
    "    )\n",
    "    \n",
    "    val_ood_dataset_traj = TrajectoryDataset(\n",
    "        ood_df,\n",
    "        seq_len=seq_len,\n",
    "        mode='ae',\n",
    "        drop_features_list=drop_features_list,\n",
    "        filter_percent = None,\n",
    "        scaler = None,\n",
    "        filter_less_seq_len = None,\n",
    "        scaler_method = 'No_Scaler'\n",
    "    )    \n",
    "    #print (val_ood_dataset_traj.inputs)\n",
    "\n",
    "    val_dataloader_traj = DataLoader(\n",
    "        val_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_ood_dataloader_traj = DataLoader(\n",
    "        val_ood_dataset_traj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    return val_dataloader_traj, val_ood_dataloader_traj, val_dataset_traj.n_features, val_dataset_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "val_dataloader_traj, val_ood_dataloader_traj, input_dim, dataset_traj = load_datasets_eval(assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ood_dataloader_traj.dataset.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader_traj.dataset.inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = EvidentialTransformerDenoiseAutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    d_model=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_g, val_aleatoric_uncertainties_g, val_epistemic_uncertainties_g, avg_aleatoric_uncertainty_g, avg_epistemic_uncertainty_g, latent_representations_eval_g, recon_error_g = evaluate_saved_model(\n",
    "    model_class=global_model, \n",
    "    model_path=global_model_save_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_dataloader_traj, \n",
    "    lambda_reg=0.5, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average:\", sum(recon_error_g) / len(recon_error_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ood_loss_g, val_ood_aleatoric_uncertainties_g, val_ood_epistemic_uncertainties_g, avg_ood_aleatoric_uncertainty_g, avg_ood_epistemic_uncertainty_g, latent_ood_representations_eval_g, recon_ood_error_g = evaluate_saved_model(\n",
    "    model_class=global_model, \n",
    "    model_path=global_model_save_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_ood_dataloader_traj, \n",
    "    lambda_reg=0.5, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average:\", sum(recon_ood_error_g) / len(recon_ood_error_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_latent_representations_g = np.concatenate([latent_representations_eval_g, latent_ood_representations_eval_g], axis = 0)\n",
    "combined_val_epistemic_uncertainties_g = np.concatenate([val_epistemic_uncertainties_g, val_ood_epistemic_uncertainties_g], axis = 0)\n",
    "combined_val_aleatoric_uncertainties_g = np.concatenate([val_aleatoric_uncertainties_g, val_ood_aleatoric_uncertainties_g], axis = 0)\n",
    "combined_recon_error_g = recon_error_g + recon_ood_error_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_labels_g = [0] * len(latent_representations_eval_g) + [1] * len(latent_ood_representations_eval_g)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_g, ood_labels_g, uncertainty_type='ood label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_g = np.percentile(recon_error_g, 95)\n",
    "print(percentile_98_g)\n",
    "plot_tsne_with_uncertainty(latent_representations_eval_g, recon_error_g, uncertainty_type='recon_error', threshold = percentile_98_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_g = np.percentile(combined_recon_error_g, 98)\n",
    "print(percentile_98_g)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_g, combined_recon_error_g, uncertainty_type='recon_error', threshold = percentile_98_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_g_uncertainty = np.percentile(combined_val_epistemic_uncertainties_g, 95)\n",
    "print(percentile_98_g_uncertainty)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_g, combined_val_epistemic_uncertainties_g, uncertainty_type='val_epistemic_uncertainties', threshold = percentile_98_g_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations_eval_g, val_epistemic_uncertainties_g, uncertainty_type='val_epistemic_uncertainties without ood', threshold = percentile_98_g_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_g_uncertainty_a = np.percentile(combined_val_aleatoric_uncertainties_g, 95)\n",
    "print(percentile_98_g_uncertainty_a)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_g, combined_val_aleatoric_uncertainties_g, uncertainty_type='val_aleatoric_uncertainties', threshold = percentile_98_g_uncertainty_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations_eval_g, val_aleatoric_uncertainties_g, uncertainty_type='val_aleatoric_uncertainties without ood', threshold = percentile_98_g_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_g, auroc_g, aupr_g, detection_error_g, threshold_g = calculate_ood_metrics(recon_error_g, recon_ood_error_g, threshold_method='percentile', percentile=95)\n",
    "print(f\"Reconstruction Error F1 score (FEAE): {f1_g:.4f}, AUROC: {auroc_g}, AUPR: {aupr_g}, Detection Error: {detection_error_g}, Threshold: {threshold_g:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_g_eu, auroc_eu, aupr_eu, detection_error_eu, threshold_g_eu = calculate_ood_metrics(val_epistemic_uncertainties_g, val_ood_epistemic_uncertainties_g, threshold_method='percentile', percentile=95)\n",
    "print(f\"Epistemic Uncertainty F1 score (FEAE): {f1_g_eu:.4f}, AUROC: {auroc_eu}, AUPR: {aupr_eu}, Detection Error: {detection_error_eu}, Threshold: {threshold_g_eu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_g_au, auroc_au, aupr_au, detection_error_au, threshold_g_au = calculate_ood_metrics(val_aleatoric_uncertainties_g, val_ood_aleatoric_uncertainties_g, threshold_method='percentile', percentile=95)\n",
    "print(f\"Aleatoric Uncertainty F1 score (FEAE): {f1_g_au:.4f}, AUROC: {auroc_au}, AUPR: {aupr_au}, Detection Error: {detection_error_au}, Threshold: {threshold_g_au:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key in val_dataset_traj.labels.keys():\n",
    "# for key in ['epoch', 'stopped', 'cog_c', 'aad', 'rot_c', 'speed_c', 'distance_c',\n",
    "#        'acc_c', 'cdd', 'dir_ccs', 'dist_ww', 'dist_ra',\n",
    "#        'dist_cl', 'dist_ma', 'traj_id', 'lon', 'lat', 'obj_id', 'datetime',\n",
    "#        'season', 'part_of_day', 'month_sin', 'month_cos', 'hour_sin',\n",
    "#        'hour_cos']:\n",
    "# for key in ['cog_c', 'aad', 'rot_c', 'speed_c', 'distance_c',\n",
    "#        'acc_c', 'cdd', 'dir_ccs', 'dist_ww', 'dist_ra',\n",
    "#        'dist_cl', 'dist_ma', 'traj_id', 'lon', 'lat',\n",
    "#        'season', 'part_of_day', 'month_sin', 'month_cos', 'hour_sin',\n",
    "#        'hour_cos']:\n",
    "#     plot_tsne_with_uncertainty(latent_representations_eval_g, dataset_traj.labels[key], uncertainty_type=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = EvidentialTransformerDenoiseAutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    d_model=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_l, val_aleatoric_uncertainties_l, val_epistemic_uncertainties_l, avg_aleatoric_uncertainty_l, avg_epistemic_uncertainty_l, latent_representations_eval_l, recon_error_l = evaluate_saved_model(\n",
    "    model_class=local_model, \n",
    "    model_path=local_model_save_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_dataloader_traj, \n",
    "    lambda_reg=0.5, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average:\", sum(recon_error_l) / len(recon_error_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ood_loss_l, val_ood_aleatoric_uncertainties_l, val_ood_epistemic_uncertainties_l, avg_ood_aleatoric_uncertainty_l, avg_ood_epistemic_uncertainty_l, latent_ood_representations_eval_l, recon_ood_error_l = evaluate_saved_model(\n",
    "    model_class=local_model, \n",
    "    model_path=local_model_save_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_ood_dataloader_traj, \n",
    "    lambda_reg=0.5, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_latent_representations_l = np.concatenate([latent_representations_eval_l, latent_ood_representations_eval_l], axis = 0)\n",
    "combined_val_epistemic_uncertainties_l = np.concatenate([val_epistemic_uncertainties_l, val_ood_epistemic_uncertainties_l], axis = 0)\n",
    "combined_val_aleatoric_uncertainties_l = np.concatenate([val_aleatoric_uncertainties_l, val_ood_aleatoric_uncertainties_l], axis = 0)\n",
    "combined_recon_error_l = recon_error_l + recon_ood_error_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_labels_l = [0] * len(latent_representations_eval_l) + [1] * len(latent_ood_representations_eval_l)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_l, ood_labels_l, uncertainty_type='ood label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_l = np.percentile(combined_recon_error_l, 95)\n",
    "print(percentile_98_l)\n",
    "\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_l, combined_recon_error_l, uncertainty_type='recon_error', threshold = percentile_98_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_l_uncertainty = np.percentile(combined_val_epistemic_uncertainties_l, 95)\n",
    "print(percentile_98_l_uncertainty)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_l, combined_val_epistemic_uncertainties_l, uncertainty_type='val_epistemic_uncertainties', threshold = percentile_98_l_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations_eval_l, val_epistemic_uncertainties_l, uncertainty_type='val_epistemic_uncertainties without ood', threshold = percentile_98_l_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_l_uncertainty_a = np.percentile(combined_val_aleatoric_uncertainties_l, 95)\n",
    "print(percentile_98_l_uncertainty_a)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_l, combined_val_aleatoric_uncertainties_l, uncertainty_type='val_aleatoric_uncertainties', threshold = percentile_98_l_uncertainty_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_uncertainty(latent_representations_eval_l, val_aleatoric_uncertainties_l, uncertainty_type='val_aleatoric_uncertainties without ood', threshold = percentile_98_l_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_l, auroc_l, aupr_l, detection_error_l, threshold_l = calculate_ood_metrics(recon_error_l, recon_ood_error_l, threshold_method='percentile', percentile=95)\n",
    "print(f\"Reconstruction Error F1 score (FEAE): {f1_l:.4f}, AUROC: {auroc_l}, AUPR: {aupr_l}, Detection Error: {detection_error_l}, Threshold: {threshold_l:.4f}\")\n",
    "\n",
    "f1_l_eu, auroc_l_eu, aupr_l_eu, detection_error_l_eu, threshold_l_eu = calculate_ood_metrics(val_epistemic_uncertainties_l, val_ood_epistemic_uncertainties_l, threshold_method='percentile', percentile=94.5)\n",
    "print(f\"Epistemic Uncertainty F1 score (FEAE): {f1_l_eu:.4f}, AUROC: {auroc_l_eu}, AUPR: {aupr_l_eu}, Detection Error: {detection_error_l_eu}, Threshold: {threshold_l_eu:.4f}\")\n",
    "\n",
    "f1_l_au, auroc_l_au, aupr_l_au, detection_error_l_au, threshold_l_au = calculate_ood_metrics(val_aleatoric_uncertainties_l, val_ood_aleatoric_uncertainties_l, threshold_method='percentile', percentile=75)\n",
    "print(f\"Aleatoric Uncertainty F1 score (FEAE): {f1_l_au:.4f}, AUROC: {auroc_l_au}, AUPR: {aupr_l_au}, Detection Error: {detection_error_l_au}, Threshold: {threshold_l_au:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = EvidentialTransformerDenoiseAutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    d_model=8,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    max_seq_length=960,\n",
    "    dropout_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_s, val_aleatoric_uncertainties_s, val_epistemic_uncertainties_s, avg_aleatoric_uncertainty_s, avg_epistemic_uncertainty_s, latent_representations_eval_s, recon_error_s = evaluate_saved_model(\n",
    "    model_class=sequential_model, \n",
    "    model_path=sequential_model_save_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_dataloader_traj, \n",
    "    lambda_reg=0.1, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ood_loss_s, val_ood_aleatoric_uncertainties_s, val_ood_epistemic_uncertainties_s, avg_ood_aleatoric_uncertainty_s, avg_ood_epistemic_uncertainty_s, latent_ood_representations_eval_s, recon_ood_error_s = evaluate_saved_model(\n",
    "    model_class=sequential_model, \n",
    "    model_path=sequential_model_save_path, \n",
    "    criterion=evidential_regression, \n",
    "    val_dataloader=val_ood_dataloader_traj, \n",
    "    lambda_reg=0.1, \n",
    "    offset=2.5, \n",
    "    device='cuda', \n",
    "    return_latent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_latent_representations_s = np.concatenate([latent_representations_eval_s, latent_ood_representations_eval_s], axis = 0)\n",
    "combined_val_epistemic_uncertainties_s = np.concatenate([val_epistemic_uncertainties_s, val_ood_epistemic_uncertainties_s], axis = 0)\n",
    "combined_val_aleatoric_uncertainties_s = np.concatenate([val_aleatoric_uncertainties_s, val_ood_aleatoric_uncertainties_s], axis = 0)\n",
    "combined_recon_error_s = recon_error_s + recon_ood_error_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_labels = [0] * len(latent_representations_eval_s) + [1] * len(latent_ood_representations_eval_s)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_s, ood_labels, uncertainty_type='ood label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_s = np.percentile(combined_recon_error_s, 98)\n",
    "print(percentile_98_s)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_s, combined_recon_error_s, uncertainty_type='recon_error', threshold = percentile_98_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_s_uncertainty = np.percentile(combined_val_epistemic_uncertainties_s, 98)\n",
    "print(percentile_98_s_uncertainty)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_s, combined_val_epistemic_uncertainties_s, uncertainty_type='val_epistemic_uncertainties', threshold = percentile_98_s_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_98_s_uncertainty_a = np.percentile(combined_val_aleatoric_uncertainties_s, 98)\n",
    "print(percentile_98_s_uncertainty_a)\n",
    "plot_tsne_with_uncertainty(combined_latent_representations_s, combined_val_aleatoric_uncertainties_s, uncertainty_type='val_aleatoric_uncertainties', threshold = percentile_98_s_uncertainty_a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
